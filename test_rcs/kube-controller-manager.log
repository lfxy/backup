I1223 12:09:23.497693    7311 leaderelection.go:188] sucessfully acquired lease kube-system/kube-controller-manager
I1223 12:09:23.498227    7311 event.go:217] Event(api.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"kube-controller-manager", UID:"e4ffb862-c907-11e6-99ee-08002750cdfb", APIVersion:"v1", ResourceVersion:"26260", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' caozq-centos10 became leader
I1223 12:09:23.504668    7311 plugins.go:94] No cloud provider specified.
W1223 12:09:23.504695    7311 controllermanager.go:285] Unsuccessful parsing of cluster CIDR : invalid CIDR address: 
W1223 12:09:23.504714    7311 controllermanager.go:289] Unsuccessful parsing of service CIDR : invalid CIDR address: 
I1223 12:09:23.504821    7311 nodecontroller.go:189] Sending events to api server.
E1223 12:09:23.505029    7311 controllermanager.go:305] Failed to start service controller: WARNING: no cloud provider provided, services of type LoadBalancer will fail.
I1223 12:09:23.505054    7311 controllermanager.go:322] Will not configure cloud provider routes for allocate-node-cidrs: false, configure-cloud-routes: true.
E1223 12:09:23.505324    7311 util.go:45] Metric for replenishment_controller already registered
E1223 12:09:23.505334    7311 util.go:45] Metric for replenishment_controller already registered
E1223 12:09:23.505339    7311 util.go:45] Metric for replenishment_controller already registered
E1223 12:09:23.505349    7311 util.go:45] Metric for replenishment_controller already registered
E1223 12:09:23.505355    7311 util.go:45] Metric for replenishment_controller already registered
I1223 12:09:23.505910    7311 replication_controller.go:219] Starting RC Manager
I1223 12:09:23.592590    7311 controllermanager.go:403] Starting extensions/v1beta1 apis
I1223 12:09:23.592621    7311 controllermanager.go:406] Starting daemon set controller
I1223 12:09:23.592875    7311 controllermanager.go:413] Starting job controller
I1223 12:09:23.593093    7311 controllermanager.go:420] Starting deployment controller
I1223 12:09:23.593293    7311 controllermanager.go:427] Starting ReplicaSet controller
I1223 12:09:23.593478    7311 controllermanager.go:436] Attempting to start horizontal pod autoscaler controller, full resource map map[apps/v1beta1:&APIResourceList{GroupVersion:apps/v1beta1,APIResources:[{statefulsets true StatefulSet} {statefulsets/status true StatefulSet}],} batch/v1:&APIResourceList{GroupVersion:batch/v1,APIResources:[{jobs true Job} {jobs/status true Job}],} policy/v1beta1:&APIResourceList{GroupVersion:policy/v1beta1,APIResources:[{poddisruptionbudgets true PodDisruptionBudget} {poddisruptionbudgets/status true PodDisruptionBudget}],} rbac.authorization.k8s.io/v1alpha1:&APIResourceList{GroupVersion:rbac.authorization.k8s.io/v1alpha1,APIResources:[{clusterrolebindings false ClusterRoleBinding} {clusterroles false ClusterRole} {rolebindings true RoleBinding} {roles true Role}],} storage.k8s.io/v1beta1:&APIResourceList{GroupVersion:storage.k8s.io/v1beta1,APIResources:[{storageclasses false StorageClass}],} v1:&APIResourceList{GroupVersion:v1,APIResources:[{bindings true Binding} {componentstatuses false ComponentStatus} {configmaps true ConfigMap} {endpoints true Endpoints} {events true Event} {limitranges true LimitRange} {namespaces false Namespace} {namespaces/finalize false Namespace} {namespaces/status false Namespace} {nodes false Node} {nodes/proxy false Node} {nodes/status false Node} {persistentvolumeclaims true PersistentVolumeClaim} {persistentvolumeclaims/status true PersistentVolumeClaim} {persistentvolumes false PersistentVolume} {persistentvolumes/status false PersistentVolume} {pods true Pod} {pods/attach true Pod} {pods/binding true Binding} {pods/eviction true Eviction} {pods/exec true Pod} {pods/log true Pod} {pods/portforward true Pod} {pods/proxy true Pod} {pods/status true Pod} {podtemplates true PodTemplate} {replicationcontrollers true ReplicationController} {replicationcontrollers/scale true Scale} {replicationcontrollers/status true ReplicationController} {resourcequotas true ResourceQuota} {resourcequotas/status true ResourceQuota} {secrets true Secret} {serviceaccounts true ServiceAccount} {services true Service} {services/proxy true Service} {services/status true Service}],} authentication.k8s.io/v1beta1:&APIResourceList{GroupVersion:authentication.k8s.io/v1beta1,APIResources:[{tokenreviews false TokenReview}],} authorization.k8s.io/v1beta1:&APIResourceList{GroupVersion:authorization.k8s.io/v1beta1,APIResources:[{localsubjectaccessreviews true LocalSubjectAccessReview} {selfsubjectaccessreviews false SelfSubjectAccessReview} {subjectaccessreviews false SubjectAccessReview}],} autoscaling/v1:&APIResourceList{GroupVersion:autoscaling/v1,APIResources:[{horizontalpodautoscalers true HorizontalPodAutoscaler} {horizontalpodautoscalers/status true HorizontalPodAutoscaler}],} certificates.k8s.io/v1alpha1:&APIResourceList{GroupVersion:certificates.k8s.io/v1alpha1,APIResources:[{certificatesigningrequests false CertificateSigningRequest} {certificatesigningrequests/approval false CertificateSigningRequest} {certificatesigningrequests/status false CertificateSigningRequest}],} extensions/v1beta1:&APIResourceList{GroupVersion:extensions/v1beta1,APIResources:[{daemonsets true DaemonSet} {daemonsets/status true DaemonSet} {deployments true Deployment} {deployments/rollback true DeploymentRollback} {deployments/scale true Scale} {deployments/status true Deployment} {horizontalpodautoscalers true HorizontalPodAutoscaler} {horizontalpodautoscalers/status true HorizontalPodAutoscaler} {ingresses true Ingress} {ingresses/status true Ingress} {jobs true Job} {jobs/status true Job} {networkpolicies true NetworkPolicy} {replicasets true ReplicaSet} {replicasets/scale true Scale} {replicasets/status true ReplicaSet} {replicationcontrollers true ReplicationControllerDummy} {replicationcontrollers/scale true Scale} {thirdpartyresources false ThirdPartyResource}],}]
E1223 12:09:23.593594    7311 controllermanager.go:437] czq HPA log!
I1223 12:09:23.593601    7311 controllermanager.go:438] 00000
I1223 12:09:23.593614    7311 controllermanager.go:444] Starting autoscaling/v1 apis
I1223 12:09:23.593625    7311 controllermanager.go:446] Starting horizontal pod controller.
I1223 12:09:23.593747    7311 controllermanager.go:464] Attempting to start disruption controller, full resource map map[extensions/v1beta1:&APIResourceList{GroupVersion:extensions/v1beta1,APIResources:[{daemonsets true DaemonSet} {daemonsets/status true DaemonSet} {deployments true Deployment} {deployments/rollback true DeploymentRollback} {deployments/scale true Scale} {deployments/status true Deployment} {horizontalpodautoscalers true HorizontalPodAutoscaler} {horizontalpodautoscalers/status true HorizontalPodAutoscaler} {ingresses true Ingress} {ingresses/status true Ingress} {jobs true Job} {jobs/status true Job} {networkpolicies true NetworkPolicy} {replicasets true ReplicaSet} {replicasets/scale true Scale} {replicasets/status true ReplicaSet} {replicationcontrollers true ReplicationControllerDummy} {replicationcontrollers/scale true Scale} {thirdpartyresources false ThirdPartyResource}],} v1:&APIResourceList{GroupVersion:v1,APIResources:[{bindings true Binding} {componentstatuses false ComponentStatus} {configmaps true ConfigMap} {endpoints true Endpoints} {events true Event} {limitranges true LimitRange} {namespaces false Namespace} {namespaces/finalize false Namespace} {namespaces/status false Namespace} {nodes false Node} {nodes/proxy false Node} {nodes/status false Node} {persistentvolumeclaims true PersistentVolumeClaim} {persistentvolumeclaims/status true PersistentVolumeClaim} {persistentvolumes false PersistentVolume} {persistentvolumes/status false PersistentVolume} {pods true Pod} {pods/attach true Pod} {pods/binding true Binding} {pods/eviction true Eviction} {pods/exec true Pod} {pods/log true Pod} {pods/portforward true Pod} {pods/proxy true Pod} {pods/status true Pod} {podtemplates true PodTemplate} {replicationcontrollers true ReplicationController} {replicationcontrollers/scale true Scale} {replicationcontrollers/status true ReplicationController} {resourcequotas true ResourceQuota} {resourcequotas/status true ResourceQuota} {secrets true Secret} {serviceaccounts true ServiceAccount} {services true Service} {services/proxy true Service} {services/status true Service}],} authentication.k8s.io/v1beta1:&APIResourceList{GroupVersion:authentication.k8s.io/v1beta1,APIResources:[{tokenreviews false TokenReview}],} authorization.k8s.io/v1beta1:&APIResourceList{GroupVersion:authorization.k8s.io/v1beta1,APIResources:[{localsubjectaccessreviews true LocalSubjectAccessReview} {selfsubjectaccessreviews false SelfSubjectAccessReview} {subjectaccessreviews false SubjectAccessReview}],} autoscaling/v1:&APIResourceList{GroupVersion:autoscaling/v1,APIResources:[{horizontalpodautoscalers true HorizontalPodAutoscaler} {horizontalpodautoscalers/status true HorizontalPodAutoscaler}],} certificates.k8s.io/v1alpha1:&APIResourceList{GroupVersion:certificates.k8s.io/v1alpha1,APIResources:[{certificatesigningrequests false CertificateSigningRequest} {certificatesigningrequests/approval false CertificateSigningRequest} {certificatesigningrequests/status false CertificateSigningRequest}],} storage.k8s.io/v1beta1:&APIResourceList{GroupVersion:storage.k8s.io/v1beta1,APIResources:[{storageclasses false StorageClass}],} apps/v1beta1:&APIResourceList{GroupVersion:apps/v1beta1,APIResources:[{statefulsets true StatefulSet} {statefulsets/status true StatefulSet}],} batch/v1:&APIResourceList{GroupVersion:batch/v1,APIResources:[{jobs true Job} {jobs/status true Job}],} policy/v1beta1:&APIResourceList{GroupVersion:policy/v1beta1,APIResources:[{poddisruptionbudgets true PodDisruptionBudget} {poddisruptionbudgets/status true PodDisruptionBudget}],} rbac.authorization.k8s.io/v1alpha1:&APIResourceList{GroupVersion:rbac.authorization.k8s.io/v1alpha1,APIResources:[{clusterrolebindings false ClusterRoleBinding} {clusterroles false ClusterRole} {rolebindings true RoleBinding} {roles true Role}],}]
I1223 12:09:23.593837    7311 controllermanager.go:466] Starting policy/v1beta1 apis
I1223 12:09:23.593848    7311 controllermanager.go:468] Starting disruption controller
I1223 12:09:23.594050    7311 controllermanager.go:476] Attempting to start statefulset, full resource map map[v1:&APIResourceList{GroupVersion:v1,APIResources:[{bindings true Binding} {componentstatuses false ComponentStatus} {configmaps true ConfigMap} {endpoints true Endpoints} {events true Event} {limitranges true LimitRange} {namespaces false Namespace} {namespaces/finalize false Namespace} {namespaces/status false Namespace} {nodes false Node} {nodes/proxy false Node} {nodes/status false Node} {persistentvolumeclaims true PersistentVolumeClaim} {persistentvolumeclaims/status true PersistentVolumeClaim} {persistentvolumes false PersistentVolume} {persistentvolumes/status false PersistentVolume} {pods true Pod} {pods/attach true Pod} {pods/binding true Binding} {pods/eviction true Eviction} {pods/exec true Pod} {pods/log true Pod} {pods/portforward true Pod} {pods/proxy true Pod} {pods/status true Pod} {podtemplates true PodTemplate} {replicationcontrollers true ReplicationController} {replicationcontrollers/scale true Scale} {replicationcontrollers/status true ReplicationController} {resourcequotas true ResourceQuota} {resourcequotas/status true ResourceQuota} {secrets true Secret} {serviceaccounts true ServiceAccount} {services true Service} {services/proxy true Service} {services/status true Service}],} authentication.k8s.io/v1beta1:&APIResourceList{GroupVersion:authentication.k8s.io/v1beta1,APIResources:[{tokenreviews false TokenReview}],} authorization.k8s.io/v1beta1:&APIResourceList{GroupVersion:authorization.k8s.io/v1beta1,APIResources:[{localsubjectaccessreviews true LocalSubjectAccessReview} {selfsubjectaccessreviews false SelfSubjectAccessReview} {subjectaccessreviews false SubjectAccessReview}],} autoscaling/v1:&APIResourceList{GroupVersion:autoscaling/v1,APIResources:[{horizontalpodautoscalers true HorizontalPodAutoscaler} {horizontalpodautoscalers/status true HorizontalPodAutoscaler}],} certificates.k8s.io/v1alpha1:&APIResourceList{GroupVersion:certificates.k8s.io/v1alpha1,APIResources:[{certificatesigningrequests false CertificateSigningRequest} {certificatesigningrequests/approval false CertificateSigningRequest} {certificatesigningrequests/status false CertificateSigningRequest}],} extensions/v1beta1:&APIResourceList{GroupVersion:extensions/v1beta1,APIResources:[{daemonsets true DaemonSet} {daemonsets/status true DaemonSet} {deployments true Deployment} {deployments/rollback true DeploymentRollback} {deployments/scale true Scale} {deployments/status true Deployment} {horizontalpodautoscalers true HorizontalPodAutoscaler} {horizontalpodautoscalers/status true HorizontalPodAutoscaler} {ingresses true Ingress} {ingresses/status true Ingress} {jobs true Job} {jobs/status true Job} {networkpolicies true NetworkPolicy} {replicasets true ReplicaSet} {replicasets/scale true Scale} {replicasets/status true ReplicaSet} {replicationcontrollers true ReplicationControllerDummy} {replicationcontrollers/scale true Scale} {thirdpartyresources false ThirdPartyResource}],} apps/v1beta1:&APIResourceList{GroupVersion:apps/v1beta1,APIResources:[{statefulsets true StatefulSet} {statefulsets/status true StatefulSet}],} batch/v1:&APIResourceList{GroupVersion:batch/v1,APIResources:[{jobs true Job} {jobs/status true Job}],} policy/v1beta1:&APIResourceList{GroupVersion:policy/v1beta1,APIResources:[{poddisruptionbudgets true PodDisruptionBudget} {poddisruptionbudgets/status true PodDisruptionBudget}],} rbac.authorization.k8s.io/v1alpha1:&APIResourceList{GroupVersion:rbac.authorization.k8s.io/v1alpha1,APIResources:[{clusterrolebindings false ClusterRoleBinding} {clusterroles false ClusterRole} {rolebindings true RoleBinding} {roles true Role}],} storage.k8s.io/v1beta1:&APIResourceList{GroupVersion:storage.k8s.io/v1beta1,APIResources:[{storageclasses false StorageClass}],}]
I1223 12:09:23.594141    7311 controllermanager.go:478] Starting apps/v1beta1 apis
I1223 12:09:23.594153    7311 controllermanager.go:480] Starting StatefulSet controller
I1223 12:09:23.594323    7311 controllermanager.go:505] Not starting batch/v2alpha1 apis
I1223 12:09:23.594672    7311 daemoncontroller.go:192] Starting Daemon Sets controller manager
I1223 12:09:23.594721    7311 deployment_controller.go:132] Starting deployment controller
I1223 12:09:23.594753    7311 replica_set.go:162] Starting ReplicaSet controller
I1223 12:09:23.594765    7311 horizontal.go:132] Starting HPA Controller
I1223 12:09:23.594988    7311 disruption.go:274] Starting disruption controller
I1223 12:09:23.594993    7311 disruption.go:276] Sending events to api server.
I1223 12:09:23.595030    7311 pet_set.go:146] Starting statefulset controller
I1223 12:09:23.669835    7311 replication_controller.go:653] Waiting for pods controller to sync, requeuing rc kube-system/php-apache
I1223 12:09:23.670168    7311 replication_controller.go:653] Waiting for pods controller to sync, requeuing rc kube-system/heapster-v1.2.0
I1223 12:09:23.670189    7311 replication_controller.go:653] Waiting for pods controller to sync, requeuing rc kube-system/kube-dns-v20
I1223 12:09:23.670201    7311 replication_controller.go:653] Waiting for pods controller to sync, requeuing rc kube-system/monitoring-influxdb-grafana-v3
I1223 12:09:23.674369    7311 controllermanager.go:543] Attempting to start certificates, full resource map map[apps/v1beta1:&APIResourceList{GroupVersion:apps/v1beta1,APIResources:[{statefulsets true StatefulSet} {statefulsets/status true StatefulSet}],} batch/v1:&APIResourceList{GroupVersion:batch/v1,APIResources:[{jobs true Job} {jobs/status true Job}],} policy/v1beta1:&APIResourceList{GroupVersion:policy/v1beta1,APIResources:[{poddisruptionbudgets true PodDisruptionBudget} {poddisruptionbudgets/status true PodDisruptionBudget}],} rbac.authorization.k8s.io/v1alpha1:&APIResourceList{GroupVersion:rbac.authorization.k8s.io/v1alpha1,APIResources:[{clusterrolebindings false ClusterRoleBinding} {clusterroles false ClusterRole} {rolebindings true RoleBinding} {roles true Role}],} storage.k8s.io/v1beta1:&APIResourceList{GroupVersion:storage.k8s.io/v1beta1,APIResources:[{storageclasses false StorageClass}],} authentication.k8s.io/v1beta1:&APIResourceList{GroupVersion:authentication.k8s.io/v1beta1,APIResources:[{tokenreviews false TokenReview}],} authorization.k8s.io/v1beta1:&APIResourceList{GroupVersion:authorization.k8s.io/v1beta1,APIResources:[{localsubjectaccessreviews true LocalSubjectAccessReview} {selfsubjectaccessreviews false SelfSubjectAccessReview} {subjectaccessreviews false SubjectAccessReview}],} autoscaling/v1:&APIResourceList{GroupVersion:autoscaling/v1,APIResources:[{horizontalpodautoscalers true HorizontalPodAutoscaler} {horizontalpodautoscalers/status true HorizontalPodAutoscaler}],} certificates.k8s.io/v1alpha1:&APIResourceList{GroupVersion:certificates.k8s.io/v1alpha1,APIResources:[{certificatesigningrequests false CertificateSigningRequest} {certificatesigningrequests/approval false CertificateSigningRequest} {certificatesigningrequests/status false CertificateSigningRequest}],} extensions/v1beta1:&APIResourceList{GroupVersion:extensions/v1beta1,APIResources:[{daemonsets true DaemonSet} {daemonsets/status true DaemonSet} {deployments true Deployment} {deployments/rollback true DeploymentRollback} {deployments/scale true Scale} {deployments/status true Deployment} {horizontalpodautoscalers true HorizontalPodAutoscaler} {horizontalpodautoscalers/status true HorizontalPodAutoscaler} {ingresses true Ingress} {ingresses/status true Ingress} {jobs true Job} {jobs/status true Job} {networkpolicies true NetworkPolicy} {replicasets true ReplicaSet} {replicasets/scale true Scale} {replicasets/status true ReplicaSet} {replicationcontrollers true ReplicationControllerDummy} {replicationcontrollers/scale true Scale} {thirdpartyresources false ThirdPartyResource}],} v1:&APIResourceList{GroupVersion:v1,APIResources:[{bindings true Binding} {componentstatuses false ComponentStatus} {configmaps true ConfigMap} {endpoints true Endpoints} {events true Event} {limitranges true LimitRange} {namespaces false Namespace} {namespaces/finalize false Namespace} {namespaces/status false Namespace} {nodes false Node} {nodes/proxy false Node} {nodes/status false Node} {persistentvolumeclaims true PersistentVolumeClaim} {persistentvolumeclaims/status true PersistentVolumeClaim} {persistentvolumes false PersistentVolume} {persistentvolumes/status false PersistentVolume} {pods true Pod} {pods/attach true Pod} {pods/binding true Binding} {pods/eviction true Eviction} {pods/exec true Pod} {pods/log true Pod} {pods/portforward true Pod} {pods/proxy true Pod} {pods/status true Pod} {podtemplates true PodTemplate} {replicationcontrollers true ReplicationController} {replicationcontrollers/scale true Scale} {replicationcontrollers/status true ReplicationController} {resourcequotas true ResourceQuota} {resourcequotas/status true ResourceQuota} {secrets true Secret} {serviceaccounts true ServiceAccount} {services true Service} {services/proxy true Service} {services/status true Service}],}]
I1223 12:09:23.674511    7311 controllermanager.go:545] Starting certificates.k8s.io/v1alpha1 apis
I1223 12:09:23.674522    7311 controllermanager.go:547] Starting certificate request controller
E1223 12:09:23.674706    7311 controllermanager.go:557] Failed to start certificate controller: open /etc/kubernetes/ca/ca.pem: no such file or directory
E1223 12:09:23.674992    7311 util.go:45] Metric for serviceaccount_controller already registered
I1223 12:09:23.675205    7311 attach_detach_controller.go:203] Starting Attach Detach Controller
I1223 12:09:23.675242    7311 serviceaccounts_controller.go:120] Starting ServiceAccount controller
I1223 12:09:23.680639    7311 garbagecollector.go:758] Garbage Collector: Initializing
I1223 12:09:23.804181    7311 replication_controller.go:653] Waiting for pods controller to sync, requeuing rc kube-system/php-apache
I1223 12:09:23.804223    7311 replication_controller.go:653] Waiting for pods controller to sync, requeuing rc kube-system/heapster-v1.2.0
I1223 12:09:23.804234    7311 replication_controller.go:653] Waiting for pods controller to sync, requeuing rc kube-system/kube-dns-v20
I1223 12:09:23.804256    7311 replication_controller.go:653] Waiting for pods controller to sync, requeuing rc kube-system/monitoring-influxdb-grafana-v3
E1223 12:09:23.827338    7311 actual_state_of_world.go:462] Failed to set statusUpdateNeeded to needed true because nodeName="10.15.137.241"  does not exist
E1223 12:09:23.827463    7311 actual_state_of_world.go:462] Failed to set statusUpdateNeeded to needed true because nodeName="10.15.137.242"  does not exist
I1223 12:09:23.873556    7311 nodecontroller.go:429] Initializing eviction metric for zone: 
W1223 12:09:23.873602    7311 nodecontroller.go:678] Missing timestamp for Node 10.15.137.241. Assuming now as a timestamp.
W1223 12:09:23.873636    7311 nodecontroller.go:678] Missing timestamp for Node 10.15.137.242. Assuming now as a timestamp.
I1223 12:09:23.873664    7311 nodecontroller.go:608] NodeController detected that zone  is now in state Normal.
I1223 12:09:23.873873    7311 event.go:217] Event(api.ObjectReference{Kind:"Node", Namespace:"", Name:"10.15.137.242", UID:"d80f4011-c8dc-11e6-b98e-08002750cdfb", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RegisteredNode' Node 10.15.137.242 event: Registered Node 10.15.137.242 in NodeController
I1223 12:09:23.873901    7311 event.go:217] Event(api.ObjectReference{Kind:"Node", Namespace:"", Name:"10.15.137.241", UID:"d5be6f5d-c8dc-11e6-b98e-08002750cdfb", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RegisteredNode' Node 10.15.137.241 event: Registered Node 10.15.137.241 in NodeController
I1223 12:09:23.948634    7311 replication_controller.go:653] Waiting for pods controller to sync, requeuing rc kube-system/php-apache
I1223 12:09:23.948723    7311 replication_controller.go:653] Waiting for pods controller to sync, requeuing rc kube-system/heapster-v1.2.0
I1223 12:09:23.948768    7311 replication_controller.go:653] Waiting for pods controller to sync, requeuing rc kube-system/kube-dns-v20
I1223 12:09:23.948860    7311 replication_controller.go:653] Waiting for pods controller to sync, requeuing rc kube-system/monitoring-influxdb-grafana-v3
E1223 12:09:32.555391    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/api/v1/watch/pods?resourceVersion=26249&timeoutSeconds=465: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.556332    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/apis/extensions/v1beta1/watch/daemonsets?resourceVersion=25602&timeoutSeconds=487: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.556407    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/apis/extensions/v1beta1/watch/deployments?resourceVersion=25602&timeoutSeconds=430: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.556446    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/apis/extensions/v1beta1/watch/horizontalpodautoscalers?resourceVersion=26118&timeoutSeconds=423: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.556481    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/apis/extensions/v1beta1/watch/ingresses?resourceVersion=25602&timeoutSeconds=384: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.556518    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/apis/extensions/v1beta1/watch/jobs?resourceVersion=25602&timeoutSeconds=338: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.556554    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/apis/extensions/v1beta1/watch/networkpolicies?resourceVersion=25602&timeoutSeconds=488: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.556602    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/apis/extensions/v1beta1/watch/replicasets?resourceVersion=25602&timeoutSeconds=371: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.556650    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/apis/extensions/v1beta1/watch/thirdpartyresources?resourceVersion=26261&timeoutSeconds=356: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.556689    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/apis/policy/v1beta1/watch/poddisruptionbudgets?resourceVersion=25602&timeoutSeconds=460: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.556730    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/apis/rbac.authorization.k8s.io/v1alpha1/watch/clusterrolebindings?resourceVersion=25602&timeoutSeconds=371: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.556797    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/apis/rbac.authorization.k8s.io/v1alpha1/watch/clusterroles?resourceVersion=25602&timeoutSeconds=410: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.556858    7311 reflector.go:300] pkg/controller/informers/factory.go:89: Failed to watch *extensions.Deployment: Get http://10.15.137.240:8080/apis/extensions/v1beta1/watch/deployments?resourceVersion=25602&timeoutSeconds=388: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.556907    7311 reflector.go:300] pkg/controller/resourcequota/resource_quota_controller.go:232: Failed to watch *api.Secret: Get http://10.15.137.240:8080/api/v1/watch/secrets?resourceVersion=25602&timeoutSeconds=525: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.556947    7311 reflector.go:300] pkg/controller/disruption/disruption.go:283: Failed to watch *api.ReplicationController: Get http://10.15.137.240:8080/api/v1/watch/replicationcontrollers?resourceVersion=26168&timeoutSeconds=592: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.557001    7311 reflector.go:300] pkg/controller/namespace/namespace_controller.go:212: Failed to watch *api.Namespace: Get http://10.15.137.240:8080/api/v1/watch/namespaces?resourceVersion=25602&timeoutSeconds=461: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.557071    7311 reflector.go:300] pkg/controller/endpoint/endpoints_controller.go:160: Failed to watch *api.Service: Get http://10.15.137.240:8080/api/v1/watch/services?resourceVersion=25602&timeoutSeconds=309: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.560474    7311 reflector.go:300] pkg/controller/replication/replication_controller.go:220: Failed to watch *api.ReplicationController: Get http://10.15.137.240:8080/api/v1/watch/replicationcontrollers?resourceVersion=26168&timeoutSeconds=508: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.560596    7311 reflector.go:300] pkg/controller/podgc/gc_controller.go:110: Failed to watch *api.Node: Get http://10.15.137.240:8080/api/v1/watch/nodes?resourceVersion=26270&timeoutSeconds=544: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.560674    7311 reflector.go:300] pkg/controller/resourcequota/resource_quota_controller.go:229: Failed to watch *api.ResourceQuota: Get http://10.15.137.240:8080/api/v1/watch/resourcequotas?resourceVersion=25602&timeoutSeconds=477: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.560808    7311 reflector.go:300] pkg/controller/serviceaccount/tokens_controller.go:170: Failed to watch *api.ServiceAccount: Get http://10.15.137.240:8080/api/v1/watch/serviceaccounts?resourceVersion=25602&timeoutSeconds=352: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.560886    7311 reflector.go:300] pkg/controller/resourcequota/resource_quota_controller.go:232: Failed to watch *api.Service: Get http://10.15.137.240:8080/api/v1/watch/services?resourceVersion=25602&timeoutSeconds=374: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.560958    7311 reflector.go:300] pkg/controller/serviceaccount/tokens_controller.go:171: Failed to watch *api.Secret: Get http://10.15.137.240:8080/api/v1/watch/secrets?fieldSelector=type%3Dkubernetes.io%2Fservice-account-token&resourceVersion=25602&timeoutSeconds=492: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.561034    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/apis/certificates.k8s.io/v1alpha1/watch/certificatesigningrequests?resourceVersion=25602&timeoutSeconds=374: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.561078    7311 reflector.go:300] pkg/controller/resourcequota/resource_quota_controller.go:232: Failed to watch *api.ReplicationController: Get http://10.15.137.240:8080/api/v1/watch/replicationcontrollers?resourceVersion=26168&timeoutSeconds=553: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.561207    7311 reflector.go:300] pkg/controller/resourcequota/resource_quota_controller.go:232: Failed to watch *api.ConfigMap: Get http://10.15.137.240:8080/api/v1/watch/configmaps?resourceVersion=25602&timeoutSeconds=307: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.561314    7311 reflector.go:300] pkg/controller/volume/persistentvolume/pv_controller_base.go:454: Failed to watch *api.PersistentVolume: Get http://10.15.137.240:8080/api/v1/watch/persistentvolumes?resourceVersion=25602&timeoutSeconds=480: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.561363    7311 reflector.go:300] pkg/controller/disruption/disruption.go:284: Failed to watch *extensions.ReplicaSet: Get http://10.15.137.240:8080/apis/extensions/v1beta1/watch/replicasets?resourceVersion=25602&timeoutSeconds=437: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.561406    7311 reflector.go:300] pkg/controller/disruption/disruption.go:285: Failed to watch *extensions.Deployment: Get http://10.15.137.240:8080/apis/extensions/v1beta1/watch/deployments?resourceVersion=25602&timeoutSeconds=563: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.561443    7311 reflector.go:300] pkg/controller/petset/pet_set.go:148: Failed to watch *apps.StatefulSet: Get http://10.15.137.240:8080/apis/apps/v1beta1/watch/statefulsets?resourceVersion=25602&timeoutSeconds=544: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.561648    7311 reflector.go:300] pkg/controller/volume/persistentvolume/pv_controller_base.go:455: Failed to watch *api.PersistentVolumeClaim: Get http://10.15.137.240:8080/api/v1/watch/persistentvolumeclaims?resourceVersion=25602&timeoutSeconds=467: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.561705    7311 reflector.go:300] pkg/controller/informers/factory.go:89: Failed to watch *api.Namespace: Get http://10.15.137.240:8080/api/v1/watch/namespaces?resourceVersion=25602&timeoutSeconds=476: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.561759    7311 reflector.go:300] pkg/controller/podautoscaler/horizontal.go:133: Failed to watch *autoscaling.HorizontalPodAutoscaler: Get http://10.15.137.240:8080/apis/autoscaling/v1/watch/horizontalpodautoscalers?resourceVersion=26118&timeoutSeconds=574: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.561798    7311 reflector.go:300] pkg/controller/informers/factory.go:89: Failed to watch *api.Node: Get http://10.15.137.240:8080/api/v1/watch/nodes?resourceVersion=26270&timeoutSeconds=300: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.568681    7311 reflector.go:300] pkg/controller/informers/factory.go:89: Failed to watch *api.PersistentVolumeClaim: Get http://10.15.137.240:8080/api/v1/watch/persistentvolumeclaims?resourceVersion=25602&timeoutSeconds=300: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.568744    7311 reflector.go:300] pkg/controller/informers/factory.go:89: Failed to watch *batch.Job: Get http://10.15.137.240:8080/apis/batch/v1/watch/jobs?resourceVersion=25602&timeoutSeconds=308: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.568787    7311 reflector.go:300] pkg/controller/volume/persistentvolume/pv_controller_base.go:159: Failed to watch *storage.StorageClass: Get http://10.15.137.240:8080/apis/storage.k8s.io/v1beta1/watch/storageclasses?resourceVersion=25602&timeoutSeconds=422: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.568841    7311 reflector.go:300] pkg/controller/informers/factory.go:89: Failed to watch *api.ServiceAccount: Get http://10.15.137.240:8080/api/v1/watch/serviceaccounts?resourceVersion=25602&timeoutSeconds=481: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.568879    7311 reflector.go:300] pkg/controller/disruption/disruption.go:281: Failed to watch *policy.PodDisruptionBudget: Get http://10.15.137.240:8080/apis/policy/v1beta1/watch/poddisruptionbudgets?resourceVersion=25602&timeoutSeconds=449: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.568921    7311 reflector.go:300] pkg/controller/informers/factory.go:89: Failed to watch *extensions.ReplicaSet: Get http://10.15.137.240:8080/apis/extensions/v1beta1/watch/replicasets?resourceVersion=25602&timeoutSeconds=419: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.568955    7311 reflector.go:300] pkg/controller/informers/factory.go:89: Failed to watch *api.PersistentVolume: Get http://10.15.137.240:8080/api/v1/watch/persistentvolumes?resourceVersion=25602&timeoutSeconds=520: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.569039    7311 reflector.go:300] pkg/controller/informers/factory.go:89: Failed to watch *api.Pod: Get http://10.15.137.240:8080/api/v1/watch/pods?resourceVersion=26249&timeoutSeconds=300: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.569079    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/apis/apps/v1beta1/watch/statefulsets?resourceVersion=25602&timeoutSeconds=549: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.569114    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/apis/autoscaling/v1/watch/horizontalpodautoscalers?resourceVersion=26118&timeoutSeconds=518: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.569183    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/apis/batch/v1/watch/jobs?resourceVersion=25602&timeoutSeconds=487: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.569665    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/apis/storage.k8s.io/v1beta1/watch/storageclasses?resourceVersion=25602&timeoutSeconds=423: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.569700    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/api/v1/watch/configmaps?resourceVersion=25602&timeoutSeconds=476: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.569745    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/api/v1/watch/endpoints?resourceVersion=26273&timeoutSeconds=471: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.569832    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/api/v1/watch/limitranges?resourceVersion=25602&timeoutSeconds=578: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.569886    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/api/v1/watch/namespaces?resourceVersion=25602&timeoutSeconds=476: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.569930    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/api/v1/watch/nodes?resourceVersion=26270&timeoutSeconds=417: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.569963    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/api/v1/watch/persistentvolumeclaims?resourceVersion=25602&timeoutSeconds=307: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.569994    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/api/v1/watch/persistentvolumes?resourceVersion=25602&timeoutSeconds=326: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.570026    7311 reflector.go:300] pkg/controller/informers/factory.go:89: Failed to watch *extensions.DaemonSet: Get http://10.15.137.240:8080/apis/extensions/v1beta1/watch/daemonsets?resourceVersion=25602&timeoutSeconds=449: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.570056    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/api/v1/watch/podtemplates?resourceVersion=25602&timeoutSeconds=488: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.570085    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/api/v1/watch/replicationcontrollers?resourceVersion=26168&timeoutSeconds=369: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.570114    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/api/v1/watch/resourcequotas?resourceVersion=25602&timeoutSeconds=549: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.570144    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/api/v1/watch/secrets?resourceVersion=25602&timeoutSeconds=406: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.570173    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/api/v1/watch/serviceaccounts?resourceVersion=25602&timeoutSeconds=345: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.570202    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/api/v1/watch/services?resourceVersion=25602&timeoutSeconds=525: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.570239    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/apis/rbac.authorization.k8s.io/v1alpha1/watch/roles?resourceVersion=25602&timeoutSeconds=500: dial tcp 10.15.137.240:8080: getsockopt: connection refused
E1223 12:09:32.570269    7311 reflector.go:300] pkg/controller/garbagecollector/garbagecollector.go:760: Failed to watch <nil>: Get http://10.15.137.240:8080/apis/rbac.authorization.k8s.io/v1alpha1/watch/rolebindings?resourceVersion=25602&timeoutSeconds=486: dial tcp 10.15.137.240:8080: getsockopt: connection refused
I1223 12:09:39.688134    7550 leaderelection.go:188] sucessfully acquired lease kube-system/kube-controller-manager
I1223 12:09:39.688624    7550 event.go:217] Event(api.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"kube-controller-manager", UID:"e4ffb862-c907-11e6-99ee-08002750cdfb", APIVersion:"v1", ResourceVersion:"26276", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' caozq-centos10 became leader
I1223 12:09:39.690452    7550 plugins.go:94] No cloud provider specified.
W1223 12:09:39.690482    7550 controllermanager.go:285] Unsuccessful parsing of cluster CIDR : invalid CIDR address: 
W1223 12:09:39.690501    7550 controllermanager.go:289] Unsuccessful parsing of service CIDR : invalid CIDR address: 
I1223 12:09:39.690597    7550 nodecontroller.go:189] Sending events to api server.
E1223 12:09:39.690847    7550 controllermanager.go:305] Failed to start service controller: WARNING: no cloud provider provided, services of type LoadBalancer will fail.
I1223 12:09:39.690869    7550 controllermanager.go:322] Will not configure cloud provider routes for allocate-node-cidrs: false, configure-cloud-routes: true.
E1223 12:09:39.692579    7550 util.go:45] Metric for replenishment_controller already registered
E1223 12:09:39.692600    7550 util.go:45] Metric for replenishment_controller already registered
E1223 12:09:39.692614    7550 util.go:45] Metric for replenishment_controller already registered
E1223 12:09:39.692635    7550 util.go:45] Metric for replenishment_controller already registered
E1223 12:09:39.692644    7550 util.go:45] Metric for replenishment_controller already registered
I1223 12:09:39.693024    7550 reflector.go:196] Starting reflector *api.Secret (0s) from pkg/controller/serviceaccount/tokens_controller.go:171
I1223 12:09:39.694763    7550 reflector.go:234] Listing and watching *api.Secret from pkg/controller/serviceaccount/tokens_controller.go:171
I1223 12:09:39.695244    7550 replication_controller.go:219] Starting RC Manager
I1223 12:09:39.695290    7550 gc_controller.go:117] PodGCController is waiting for informer sync...
I1223 12:09:39.695397    7550 resource_quota_controller.go:153] Resource quota controller queued all resource quota for full calculation of usage
I1223 12:09:39.695495    7550 reflector.go:196] Starting reflector *api.ServiceAccount (0s) from pkg/controller/serviceaccount/tokens_controller.go:170
I1223 12:09:39.695508    7550 reflector.go:234] Listing and watching *api.ServiceAccount from pkg/controller/serviceaccount/tokens_controller.go:170
I1223 12:09:39.695793    7550 reflector.go:196] Starting reflector *api.Service (30s) from pkg/controller/endpoint/endpoints_controller.go:160
I1223 12:09:39.695809    7550 reflector.go:234] Listing and watching *api.Service from pkg/controller/endpoint/endpoints_controller.go:160
I1223 12:09:39.696349    7550 reflector.go:196] Starting reflector *api.ReplicationController (10m0s) from pkg/controller/replication/replication_controller.go:220
I1223 12:09:39.696369    7550 reflector.go:234] Listing and watching *api.ReplicationController from pkg/controller/replication/replication_controller.go:220
I1223 12:09:39.696618    7550 reflector.go:196] Starting reflector *api.Node (0s) from pkg/controller/podgc/gc_controller.go:110
I1223 12:09:39.696641    7550 reflector.go:234] Listing and watching *api.Node from pkg/controller/podgc/gc_controller.go:110
I1223 12:09:39.696912    7550 reflector.go:196] Starting reflector *api.ResourceQuota (5m0s) from pkg/controller/resourcequota/resource_quota_controller.go:229
I1223 12:09:39.696927    7550 reflector.go:234] Listing and watching *api.ResourceQuota from pkg/controller/resourcequota/resource_quota_controller.go:229
I1223 12:09:39.697377    7550 reflector.go:196] Starting reflector *api.Service (13h9m49.083217105s) from pkg/controller/resourcequota/resource_quota_controller.go:232
I1223 12:09:39.697407    7550 reflector.go:234] Listing and watching *api.Service from pkg/controller/resourcequota/resource_quota_controller.go:232
I1223 12:09:39.697645    7550 reflector.go:196] Starting reflector *api.ReplicationController (15h36m39.392377284s) from pkg/controller/resourcequota/resource_quota_controller.go:232
I1223 12:09:39.697659    7550 reflector.go:234] Listing and watching *api.ReplicationController from pkg/controller/resourcequota/resource_quota_controller.go:232
I1223 12:09:39.697915    7550 reflector.go:196] Starting reflector *api.Secret (18h10m57.185551289s) from pkg/controller/resourcequota/resource_quota_controller.go:232
I1223 12:09:39.697945    7550 reflector.go:234] Listing and watching *api.Secret from pkg/controller/resourcequota/resource_quota_controller.go:232
I1223 12:09:39.698149    7550 reflector.go:196] Starting reflector *api.ConfigMap (21h45m49.246314772s) from pkg/controller/resourcequota/resource_quota_controller.go:232
I1223 12:09:39.698163    7550 reflector.go:234] Listing and watching *api.ConfigMap from pkg/controller/resourcequota/resource_quota_controller.go:232
I1223 12:09:39.724263    7550 controllermanager.go:403] Starting extensions/v1beta1 apis
I1223 12:09:39.724294    7550 controllermanager.go:406] Starting daemon set controller
I1223 12:09:39.724532    7550 controllermanager.go:413] Starting job controller
I1223 12:09:39.724728    7550 controllermanager.go:420] Starting deployment controller
I1223 12:09:39.724931    7550 controllermanager.go:427] Starting ReplicaSet controller
I1223 12:09:39.725127    7550 controllermanager.go:436] Attempting to start horizontal pod autoscaler controller, full resource map map[extensions/v1beta1:&APIResourceList{GroupVersion:extensions/v1beta1,APIResources:[{daemonsets true DaemonSet} {daemonsets/status true DaemonSet} {deployments true Deployment} {deployments/rollback true DeploymentRollback} {deployments/scale true Scale} {deployments/status true Deployment} {horizontalpodautoscalers true HorizontalPodAutoscaler} {horizontalpodautoscalers/status true HorizontalPodAutoscaler} {ingresses true Ingress} {ingresses/status true Ingress} {jobs true Job} {jobs/status true Job} {networkpolicies true NetworkPolicy} {replicasets true ReplicaSet} {replicasets/scale true Scale} {replicasets/status true ReplicaSet} {replicationcontrollers true ReplicationControllerDummy} {replicationcontrollers/scale true Scale} {thirdpartyresources false ThirdPartyResource}],} policy/v1beta1:&APIResourceList{GroupVersion:policy/v1beta1,APIResources:[{poddisruptionbudgets true PodDisruptionBudget} {poddisruptionbudgets/status true PodDisruptionBudget}],} v1:&APIResourceList{GroupVersion:v1,APIResources:[{bindings true Binding} {componentstatuses false ComponentStatus} {configmaps true ConfigMap} {endpoints true Endpoints} {events true Event} {limitranges true LimitRange} {namespaces false Namespace} {namespaces/finalize false Namespace} {namespaces/status false Namespace} {nodes false Node} {nodes/proxy false Node} {nodes/status false Node} {persistentvolumeclaims true PersistentVolumeClaim} {persistentvolumeclaims/status true PersistentVolumeClaim} {persistentvolumes false PersistentVolume} {persistentvolumes/status false PersistentVolume} {pods true Pod} {pods/attach true Pod} {pods/binding true Binding} {pods/eviction true Eviction} {pods/exec true Pod} {pods/log true Pod} {pods/portforward true Pod} {pods/proxy true Pod} {pods/status true Pod} {podtemplates true PodTemplate} {replicationcontrollers true ReplicationController} {replicationcontrollers/scale true Scale} {replicationcontrollers/status true ReplicationController} {resourcequotas true ResourceQuota} {resourcequotas/status true ResourceQuota} {secrets true Secret} {serviceaccounts true ServiceAccount} {services true Service} {services/proxy true Service} {services/status true Service}],} apps/v1beta1:&APIResourceList{GroupVersion:apps/v1beta1,APIResources:[{statefulsets true StatefulSet} {statefulsets/status true StatefulSet}],} authentication.k8s.io/v1beta1:&APIResourceList{GroupVersion:authentication.k8s.io/v1beta1,APIResources:[{tokenreviews false TokenReview}],} authorization.k8s.io/v1beta1:&APIResourceList{GroupVersion:authorization.k8s.io/v1beta1,APIResources:[{localsubjectaccessreviews true LocalSubjectAccessReview} {selfsubjectaccessreviews false SelfSubjectAccessReview} {subjectaccessreviews false SubjectAccessReview}],} autoscaling/v1:&APIResourceList{GroupVersion:autoscaling/v1,APIResources:[{horizontalpodautoscalers true HorizontalPodAutoscaler} {horizontalpodautoscalers/status true HorizontalPodAutoscaler}],} certificates.k8s.io/v1alpha1:&APIResourceList{GroupVersion:certificates.k8s.io/v1alpha1,APIResources:[{certificatesigningrequests false CertificateSigningRequest} {certificatesigningrequests/approval false CertificateSigningRequest} {certificatesigningrequests/status false CertificateSigningRequest}],} batch/v1:&APIResourceList{GroupVersion:batch/v1,APIResources:[{jobs true Job} {jobs/status true Job}],} rbac.authorization.k8s.io/v1alpha1:&APIResourceList{GroupVersion:rbac.authorization.k8s.io/v1alpha1,APIResources:[{clusterrolebindings false ClusterRoleBinding} {clusterroles false ClusterRole} {rolebindings true RoleBinding} {roles true Role}],} storage.k8s.io/v1beta1:&APIResourceList{GroupVersion:storage.k8s.io/v1beta1,APIResources:[{storageclasses false StorageClass}],}]
E1223 12:09:39.725252    7550 controllermanager.go:437] czq HPA log!
I1223 12:09:39.725259    7550 controllermanager.go:438] 00000
I1223 12:09:39.725265    7550 controllermanager.go:439] 11111
I1223 12:09:39.725270    7550 controllermanager.go:440] 22222
I1223 12:09:39.725275    7550 controllermanager.go:441] 3333
I1223 12:09:39.725281    7550 controllermanager.go:442] 44444
I1223 12:09:39.725293    7550 controllermanager.go:444] Starting autoscaling/v1 apis
I1223 12:09:39.725304    7550 controllermanager.go:446] Starting horizontal pod controller.
I1223 12:09:39.725437    7550 controllermanager.go:464] Attempting to start disruption controller, full resource map map[storage.k8s.io/v1beta1:&APIResourceList{GroupVersion:storage.k8s.io/v1beta1,APIResources:[{storageclasses false StorageClass}],} batch/v1:&APIResourceList{GroupVersion:batch/v1,APIResources:[{jobs true Job} {jobs/status true Job}],} rbac.authorization.k8s.io/v1alpha1:&APIResourceList{GroupVersion:rbac.authorization.k8s.io/v1alpha1,APIResources:[{clusterrolebindings false ClusterRoleBinding} {clusterroles false ClusterRole} {rolebindings true RoleBinding} {roles true Role}],} authorization.k8s.io/v1beta1:&APIResourceList{GroupVersion:authorization.k8s.io/v1beta1,APIResources:[{localsubjectaccessreviews true LocalSubjectAccessReview} {selfsubjectaccessreviews false SelfSubjectAccessReview} {subjectaccessreviews false SubjectAccessReview}],} autoscaling/v1:&APIResourceList{GroupVersion:autoscaling/v1,APIResources:[{horizontalpodautoscalers true HorizontalPodAutoscaler} {horizontalpodautoscalers/status true HorizontalPodAutoscaler}],} certificates.k8s.io/v1alpha1:&APIResourceList{GroupVersion:certificates.k8s.io/v1alpha1,APIResources:[{certificatesigningrequests false CertificateSigningRequest} {certificatesigningrequests/approval false CertificateSigningRequest} {certificatesigningrequests/status false CertificateSigningRequest}],} extensions/v1beta1:&APIResourceList{GroupVersion:extensions/v1beta1,APIResources:[{daemonsets true DaemonSet} {daemonsets/status true DaemonSet} {deployments true Deployment} {deployments/rollback true DeploymentRollback} {deployments/scale true Scale} {deployments/status true Deployment} {horizontalpodautoscalers true HorizontalPodAutoscaler} {horizontalpodautoscalers/status true HorizontalPodAutoscaler} {ingresses true Ingress} {ingresses/status true Ingress} {jobs true Job} {jobs/status true Job} {networkpolicies true NetworkPolicy} {replicasets true ReplicaSet} {replicasets/scale true Scale} {replicasets/status true ReplicaSet} {replicationcontrollers true ReplicationControllerDummy} {replicationcontrollers/scale true Scale} {thirdpartyresources false ThirdPartyResource}],} policy/v1beta1:&APIResourceList{GroupVersion:policy/v1beta1,APIResources:[{poddisruptionbudgets true PodDisruptionBudget} {poddisruptionbudgets/status true PodDisruptionBudget}],} v1:&APIResourceList{GroupVersion:v1,APIResources:[{bindings true Binding} {componentstatuses false ComponentStatus} {configmaps true ConfigMap} {endpoints true Endpoints} {events true Event} {limitranges true LimitRange} {namespaces false Namespace} {namespaces/finalize false Namespace} {namespaces/status false Namespace} {nodes false Node} {nodes/proxy false Node} {nodes/status false Node} {persistentvolumeclaims true PersistentVolumeClaim} {persistentvolumeclaims/status true PersistentVolumeClaim} {persistentvolumes false PersistentVolume} {persistentvolumes/status false PersistentVolume} {pods true Pod} {pods/attach true Pod} {pods/binding true Binding} {pods/eviction true Eviction} {pods/exec true Pod} {pods/log true Pod} {pods/portforward true Pod} {pods/proxy true Pod} {pods/status true Pod} {podtemplates true PodTemplate} {replicationcontrollers true ReplicationController} {replicationcontrollers/scale true Scale} {replicationcontrollers/status true ReplicationController} {resourcequotas true ResourceQuota} {resourcequotas/status true ResourceQuota} {secrets true Secret} {serviceaccounts true ServiceAccount} {services true Service} {services/proxy true Service} {services/status true Service}],} apps/v1beta1:&APIResourceList{GroupVersion:apps/v1beta1,APIResources:[{statefulsets true StatefulSet} {statefulsets/status true StatefulSet}],} authentication.k8s.io/v1beta1:&APIResourceList{GroupVersion:authentication.k8s.io/v1beta1,APIResources:[{tokenreviews false TokenReview}],}]
I1223 12:09:39.725534    7550 controllermanager.go:466] Starting policy/v1beta1 apis
I1223 12:09:39.725546    7550 controllermanager.go:468] Starting disruption controller
I1223 12:09:39.725857    7550 controllermanager.go:476] Attempting to start statefulset, full resource map map[rbac.authorization.k8s.io/v1alpha1:&APIResourceList{GroupVersion:rbac.authorization.k8s.io/v1alpha1,APIResources:[{clusterrolebindings false ClusterRoleBinding} {clusterroles false ClusterRole} {rolebindings true RoleBinding} {roles true Role}],} storage.k8s.io/v1beta1:&APIResourceList{GroupVersion:storage.k8s.io/v1beta1,APIResources:[{storageclasses false StorageClass}],} batch/v1:&APIResourceList{GroupVersion:batch/v1,APIResources:[{jobs true Job} {jobs/status true Job}],} authentication.k8s.io/v1beta1:&APIResourceList{GroupVersion:authentication.k8s.io/v1beta1,APIResources:[{tokenreviews false TokenReview}],} authorization.k8s.io/v1beta1:&APIResourceList{GroupVersion:authorization.k8s.io/v1beta1,APIResources:[{localsubjectaccessreviews true LocalSubjectAccessReview} {selfsubjectaccessreviews false SelfSubjectAccessReview} {subjectaccessreviews false SubjectAccessReview}],} autoscaling/v1:&APIResourceList{GroupVersion:autoscaling/v1,APIResources:[{horizontalpodautoscalers true HorizontalPodAutoscaler} {horizontalpodautoscalers/status true HorizontalPodAutoscaler}],} certificates.k8s.io/v1alpha1:&APIResourceList{GroupVersion:certificates.k8s.io/v1alpha1,APIResources:[{certificatesigningrequests false CertificateSigningRequest} {certificatesigningrequests/approval false CertificateSigningRequest} {certificatesigningrequests/status false CertificateSigningRequest}],} extensions/v1beta1:&APIResourceList{GroupVersion:extensions/v1beta1,APIResources:[{daemonsets true DaemonSet} {daemonsets/status true DaemonSet} {deployments true Deployment} {deployments/rollback true DeploymentRollback} {deployments/scale true Scale} {deployments/status true Deployment} {horizontalpodautoscalers true HorizontalPodAutoscaler} {horizontalpodautoscalers/status true HorizontalPodAutoscaler} {ingresses true Ingress} {ingresses/status true Ingress} {jobs true Job} {jobs/status true Job} {networkpolicies true NetworkPolicy} {replicasets true ReplicaSet} {replicasets/scale true Scale} {replicasets/status true ReplicaSet} {replicationcontrollers true ReplicationControllerDummy} {replicationcontrollers/scale true Scale} {thirdpartyresources false ThirdPartyResource}],} policy/v1beta1:&APIResourceList{GroupVersion:policy/v1beta1,APIResources:[{poddisruptionbudgets true PodDisruptionBudget} {poddisruptionbudgets/status true PodDisruptionBudget}],} v1:&APIResourceList{GroupVersion:v1,APIResources:[{bindings true Binding} {componentstatuses false ComponentStatus} {configmaps true ConfigMap} {endpoints true Endpoints} {events true Event} {limitranges true LimitRange} {namespaces false Namespace} {namespaces/finalize false Namespace} {namespaces/status false Namespace} {nodes false Node} {nodes/proxy false Node} {nodes/status false Node} {persistentvolumeclaims true PersistentVolumeClaim} {persistentvolumeclaims/status true PersistentVolumeClaim} {persistentvolumes false PersistentVolume} {persistentvolumes/status false PersistentVolume} {pods true Pod} {pods/attach true Pod} {pods/binding true Binding} {pods/eviction true Eviction} {pods/exec true Pod} {pods/log true Pod} {pods/portforward true Pod} {pods/proxy true Pod} {pods/status true Pod} {podtemplates true PodTemplate} {replicationcontrollers true ReplicationController} {replicationcontrollers/scale true Scale} {replicationcontrollers/status true ReplicationController} {resourcequotas true ResourceQuota} {resourcequotas/status true ResourceQuota} {secrets true Secret} {serviceaccounts true ServiceAccount} {services true Service} {services/proxy true Service} {services/status true Service}],} apps/v1beta1:&APIResourceList{GroupVersion:apps/v1beta1,APIResources:[{statefulsets true StatefulSet} {statefulsets/status true StatefulSet}],}]
I1223 12:09:39.725960    7550 controllermanager.go:478] Starting apps/v1beta1 apis
I1223 12:09:39.725972    7550 controllermanager.go:480] Starting StatefulSet controller
I1223 12:09:39.726157    7550 controllermanager.go:505] Not starting batch/v2alpha1 apis
I1223 12:09:39.726389    7550 plugins.go:344] Loaded volume plugin "kubernetes.io/host-path"
I1223 12:09:39.726402    7550 plugins.go:344] Loaded volume plugin "kubernetes.io/nfs"
I1223 12:09:39.726409    7550 plugins.go:344] Loaded volume plugin "kubernetes.io/glusterfs"
I1223 12:09:39.726416    7550 plugins.go:344] Loaded volume plugin "kubernetes.io/rbd"
I1223 12:09:39.726423    7550 plugins.go:344] Loaded volume plugin "kubernetes.io/quobyte"
I1223 12:09:39.726430    7550 plugins.go:344] Loaded volume plugin "kubernetes.io/flocker"
I1223 12:09:39.726454    7550 pv_controller_base.go:452] starting PersistentVolumeController
I1223 12:09:39.726685    7550 daemoncontroller.go:192] Starting Daemon Sets controller manager
I1223 12:09:39.726745    7550 deployment_controller.go:132] Starting deployment controller
I1223 12:09:39.726768    7550 replica_set.go:162] Starting ReplicaSet controller
I1223 12:09:39.726781    7550 horizontal.go:132] Starting HPA Controller
I1223 12:09:39.726876    7550 reflector.go:196] Starting reflector *autoscaling.HorizontalPodAutoscaler (30s) from pkg/controller/podautoscaler/horizontal.go:133
I1223 12:09:39.726894    7550 reflector.go:234] Listing and watching *autoscaling.HorizontalPodAutoscaler from pkg/controller/podautoscaler/horizontal.go:133
I1223 12:09:39.728511    7550 disruption.go:274] Starting disruption controller
I1223 12:09:39.728536    7550 disruption.go:276] Sending events to api server.
I1223 12:09:39.728605    7550 pet_set.go:146] Starting statefulset controller
I1223 12:09:39.728721    7550 reflector.go:196] Starting reflector *api.Namespace (5m0s) from pkg/controller/namespace/namespace_controller.go:212
I1223 12:09:39.728738    7550 reflector.go:234] Listing and watching *api.Namespace from pkg/controller/namespace/namespace_controller.go:212
I1223 12:09:39.729117    7550 reflector.go:196] Starting reflector *policy.PodDisruptionBudget (30s) from pkg/controller/disruption/disruption.go:281
I1223 12:09:39.729139    7550 reflector.go:234] Listing and watching *policy.PodDisruptionBudget from pkg/controller/disruption/disruption.go:281
I1223 12:09:39.729394    7550 reflector.go:196] Starting reflector *api.ReplicationController (30s) from pkg/controller/disruption/disruption.go:283
I1223 12:09:39.729424    7550 reflector.go:234] Listing and watching *api.ReplicationController from pkg/controller/disruption/disruption.go:283
I1223 12:09:39.729623    7550 reflector.go:196] Starting reflector *extensions.ReplicaSet (30s) from pkg/controller/disruption/disruption.go:284
I1223 12:09:39.729637    7550 reflector.go:234] Listing and watching *extensions.ReplicaSet from pkg/controller/disruption/disruption.go:284
I1223 12:09:39.729917    7550 reflector.go:196] Starting reflector *extensions.Deployment (30s) from pkg/controller/disruption/disruption.go:285
I1223 12:09:39.729948    7550 reflector.go:234] Listing and watching *extensions.Deployment from pkg/controller/disruption/disruption.go:285
I1223 12:09:39.730287    7550 reflector.go:196] Starting reflector *apps.StatefulSet (30s) from pkg/controller/petset/pet_set.go:148
I1223 12:09:39.730302    7550 reflector.go:234] Listing and watching *apps.StatefulSet from pkg/controller/petset/pet_set.go:148
I1223 12:09:39.733975    7550 pv_controller_base.go:210] controller initialized
I1223 12:09:39.734721    7550 plugins.go:344] Loaded volume plugin "kubernetes.io/aws-ebs"
I1223 12:09:39.734735    7550 plugins.go:344] Loaded volume plugin "kubernetes.io/gce-pd"
I1223 12:09:39.734744    7550 plugins.go:344] Loaded volume plugin "kubernetes.io/cinder"
I1223 12:09:39.734752    7550 plugins.go:344] Loaded volume plugin "kubernetes.io/vsphere-volume"
I1223 12:09:39.734759    7550 plugins.go:344] Loaded volume plugin "kubernetes.io/azure-disk"
I1223 12:09:39.734767    7550 plugins.go:344] Loaded volume plugin "kubernetes.io/photon-pd"
I1223 12:09:39.734788    7550 controllermanager.go:543] Attempting to start certificates, full resource map map[v1:&APIResourceList{GroupVersion:v1,APIResources:[{bindings true Binding} {componentstatuses false ComponentStatus} {configmaps true ConfigMap} {endpoints true Endpoints} {events true Event} {limitranges true LimitRange} {namespaces false Namespace} {namespaces/finalize false Namespace} {namespaces/status false Namespace} {nodes false Node} {nodes/proxy false Node} {nodes/status false Node} {persistentvolumeclaims true PersistentVolumeClaim} {persistentvolumeclaims/status true PersistentVolumeClaim} {persistentvolumes false PersistentVolume} {persistentvolumes/status false PersistentVolume} {pods true Pod} {pods/attach true Pod} {pods/binding true Binding} {pods/eviction true Eviction} {pods/exec true Pod} {pods/log true Pod} {pods/portforward true Pod} {pods/proxy true Pod} {pods/status true Pod} {podtemplates true PodTemplate} {replicationcontrollers true ReplicationController} {replicationcontrollers/scale true Scale} {replicationcontrollers/status true ReplicationController} {resourcequotas true ResourceQuota} {resourcequotas/status true ResourceQuota} {secrets true Secret} {serviceaccounts true ServiceAccount} {services true Service} {services/proxy true Service} {services/status true Service}],} apps/v1beta1:&APIResourceList{GroupVersion:apps/v1beta1,APIResources:[{statefulsets true StatefulSet} {statefulsets/status true StatefulSet}],} authentication.k8s.io/v1beta1:&APIResourceList{GroupVersion:authentication.k8s.io/v1beta1,APIResources:[{tokenreviews false TokenReview}],} authorization.k8s.io/v1beta1:&APIResourceList{GroupVersion:authorization.k8s.io/v1beta1,APIResources:[{localsubjectaccessreviews true LocalSubjectAccessReview} {selfsubjectaccessreviews false SelfSubjectAccessReview} {subjectaccessreviews false SubjectAccessReview}],} autoscaling/v1:&APIResourceList{GroupVersion:autoscaling/v1,APIResources:[{horizontalpodautoscalers true HorizontalPodAutoscaler} {horizontalpodautoscalers/status true HorizontalPodAutoscaler}],} certificates.k8s.io/v1alpha1:&APIResourceList{GroupVersion:certificates.k8s.io/v1alpha1,APIResources:[{certificatesigningrequests false CertificateSigningRequest} {certificatesigningrequests/approval false CertificateSigningRequest} {certificatesigningrequests/status false CertificateSigningRequest}],} extensions/v1beta1:&APIResourceList{GroupVersion:extensions/v1beta1,APIResources:[{daemonsets true DaemonSet} {daemonsets/status true DaemonSet} {deployments true Deployment} {deployments/rollback true DeploymentRollback} {deployments/scale true Scale} {deployments/status true Deployment} {horizontalpodautoscalers true HorizontalPodAutoscaler} {horizontalpodautoscalers/status true HorizontalPodAutoscaler} {ingresses true Ingress} {ingresses/status true Ingress} {jobs true Job} {jobs/status true Job} {networkpolicies true NetworkPolicy} {replicasets true ReplicaSet} {replicasets/scale true Scale} {replicasets/status true ReplicaSet} {replicationcontrollers true ReplicationControllerDummy} {replicationcontrollers/scale true Scale} {thirdpartyresources false ThirdPartyResource}],} policy/v1beta1:&APIResourceList{GroupVersion:policy/v1beta1,APIResources:[{poddisruptionbudgets true PodDisruptionBudget} {poddisruptionbudgets/status true PodDisruptionBudget}],} batch/v1:&APIResourceList{GroupVersion:batch/v1,APIResources:[{jobs true Job} {jobs/status true Job}],} rbac.authorization.k8s.io/v1alpha1:&APIResourceList{GroupVersion:rbac.authorization.k8s.io/v1alpha1,APIResources:[{clusterrolebindings false ClusterRoleBinding} {clusterroles false ClusterRole} {rolebindings true RoleBinding} {roles true Role}],} storage.k8s.io/v1beta1:&APIResourceList{GroupVersion:storage.k8s.io/v1beta1,APIResources:[{storageclasses false StorageClass}],}]
I1223 12:09:39.734906    7550 controllermanager.go:545] Starting certificates.k8s.io/v1alpha1 apis
I1223 12:09:39.734917    7550 controllermanager.go:547] Starting certificate request controller
E1223 12:09:39.735086    7550 controllermanager.go:557] Failed to start certificate controller: open /etc/kubernetes/ca/ca.pem: no such file or directory
E1223 12:09:39.735335    7550 util.go:45] Metric for serviceaccount_controller already registered
I1223 12:09:39.735651    7550 reflector.go:196] Starting reflector *api.PersistentVolume (15s) from pkg/controller/volume/persistentvolume/pv_controller_base.go:454
I1223 12:09:39.735671    7550 reflector.go:234] Listing and watching *api.PersistentVolume from pkg/controller/volume/persistentvolume/pv_controller_base.go:454
I1223 12:09:39.736068    7550 reflector.go:196] Starting reflector *api.PersistentVolumeClaim (15s) from pkg/controller/volume/persistentvolume/pv_controller_base.go:455
I1223 12:09:39.736094    7550 reflector.go:234] Listing and watching *api.PersistentVolumeClaim from pkg/controller/volume/persistentvolume/pv_controller_base.go:455
I1223 12:09:39.736286    7550 reflector.go:196] Starting reflector *storage.StorageClass (15s) from pkg/controller/volume/persistentvolume/pv_controller_base.go:159
I1223 12:09:39.736313    7550 reflector.go:234] Listing and watching *storage.StorageClass from pkg/controller/volume/persistentvolume/pv_controller_base.go:159
I1223 12:09:39.736459    7550 attach_detach_controller.go:203] Starting Attach Detach Controller
I1223 12:09:39.736493    7550 serviceaccounts_controller.go:120] Starting ServiceAccount controller
I1223 12:09:39.746606    7550 reflector.go:196] Starting reflector *extensions.ReplicaSet (23h17m9.992603544s) from pkg/controller/informers/factory.go:89
I1223 12:09:39.746636    7550 reflector.go:234] Listing and watching *extensions.ReplicaSet from pkg/controller/informers/factory.go:89
I1223 12:09:39.746820    7550 garbagecollector.go:758] Garbage Collector: Initializing
I1223 12:09:39.746951    7550 reflector.go:196] Starting reflector *api.Namespace (23h17m9.992603544s) from pkg/controller/informers/factory.go:89
I1223 12:09:39.746972    7550 reflector.go:234] Listing and watching *api.Namespace from pkg/controller/informers/factory.go:89
I1223 12:09:39.747176    7550 reflector.go:196] Starting reflector *api.Pod (23h17m9.992603544s) from pkg/controller/informers/factory.go:89
I1223 12:09:39.747231    7550 reflector.go:234] Listing and watching *api.Pod from pkg/controller/informers/factory.go:89
I1223 12:09:39.747539    7550 reflector.go:196] Starting reflector *api.Node (23h17m9.992603544s) from pkg/controller/informers/factory.go:89
I1223 12:09:39.747565    7550 reflector.go:234] Listing and watching *api.Node from pkg/controller/informers/factory.go:89
I1223 12:09:39.747811    7550 reflector.go:196] Starting reflector *extensions.Deployment (30s) from pkg/controller/informers/factory.go:89
I1223 12:09:39.747827    7550 reflector.go:234] Listing and watching *extensions.Deployment from pkg/controller/informers/factory.go:89
I1223 12:09:39.748106    7550 reflector.go:196] Starting reflector *api.PersistentVolume (23h17m9.992603544s) from pkg/controller/informers/factory.go:89
I1223 12:09:39.748157    7550 reflector.go:234] Listing and watching *api.PersistentVolume from pkg/controller/informers/factory.go:89
I1223 12:09:39.748382    7550 reflector.go:196] Starting reflector *api.ServiceAccount (23h17m9.992603544s) from pkg/controller/informers/factory.go:89
I1223 12:09:39.748396    7550 reflector.go:234] Listing and watching *api.ServiceAccount from pkg/controller/informers/factory.go:89
I1223 12:09:39.748662    7550 reflector.go:196] Starting reflector *extensions.DaemonSet (23h17m9.992603544s) from pkg/controller/informers/factory.go:89
I1223 12:09:39.748679    7550 reflector.go:234] Listing and watching *extensions.DaemonSet from pkg/controller/informers/factory.go:89
I1223 12:09:39.748936    7550 reflector.go:196] Starting reflector *api.PersistentVolumeClaim (23h17m9.992603544s) from pkg/controller/informers/factory.go:89
I1223 12:09:39.748952    7550 reflector.go:234] Listing and watching *api.PersistentVolumeClaim from pkg/controller/informers/factory.go:89
I1223 12:09:39.749196    7550 reflector.go:196] Starting reflector *batch.Job (23h17m9.992603544s) from pkg/controller/informers/factory.go:89
I1223 12:09:39.749218    7550 reflector.go:234] Listing and watching *batch.Job from pkg/controller/informers/factory.go:89
I1223 12:09:39.749460    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.749474    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.749704    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.749721    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.749976    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.750000    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.751756    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.751796    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.752048    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.752063    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.752469    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.752490    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.752716    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.752740    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.752979    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.752992    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.753192    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.753213    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.753425    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.753442    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.753657    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.753679    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.753909    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.753923    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.754143    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.754156    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.754371    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.754384    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.754596    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.754618    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.754836    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.754849    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.755052    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.755066    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.755276    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.755289    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.755503    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.755527    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.755748    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.755762    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.755968    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.755981    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.756894    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.756920    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.757181    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.757206    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.757459    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.757482    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.757697    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.757711    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.757930    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.757944    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.758147    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.758172    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.758410    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.758427    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.758646    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.758664    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.758877    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.758891    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.759092    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.759113    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.759325    7550 reflector.go:196] Starting reflector <nil> (0s) from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.759343    7550 reflector.go:234] Listing and watching <nil> from pkg/controller/garbagecollector/garbagecollector.go:760
I1223 12:09:39.762511    7550 deployment_controller.go:180] ReplicaSet sample-deployment-3552496463 added.
I1223 12:09:39.762542    7550 deployment_controller.go:190] Error: could not find deployments set for ReplicaSet sample-deployment-3552496463 in namespace default with labels: map[app:sample-deployment pod-template-hash:3552496463]. No deployment found for ReplicaSet sample-deployment-3552496463, deployment controller will avoid syncing.
I1223 12:09:39.780640    7550 attach_detach_controller.go:539] processVolumesInUse for node "10.15.137.241"
E1223 12:09:39.780673    7550 actual_state_of_world.go:462] Failed to set statusUpdateNeeded to needed true because nodeName="10.15.137.241"  does not exist
I1223 12:09:39.780679    7550 attach_detach_controller.go:539] processVolumesInUse for node "10.15.137.242"
E1223 12:09:39.780683    7550 actual_state_of_world.go:462] Failed to set statusUpdateNeeded to needed true because nodeName="10.15.137.242"  does not exist
I1223 12:09:39.780754    7550 deployment_controller.go:148] Adding deployment sample-deployment
I1223 12:09:39.781102    7550 replication_controller.go:255] No controllers found for pod sample-deployment-3552496463-0tevs, replication manager will avoid syncing
I1223 12:09:39.781116    7550 replication_controller.go:255] No controllers found for pod busybox, replication manager will avoid syncing
I1223 12:09:39.781156    7550 daemoncontroller.go:305] Pod kube-dns-v20-xojuw added.
I1223 12:09:39.781230    7550 daemoncontroller.go:257] No daemon sets found for pod kube-dns-v20-xojuw, daemon set controller will avoid syncing
I1223 12:09:39.781240    7550 daemoncontroller.go:305] Pod monitoring-influxdb-grafana-v3-n3odm added.
I1223 12:09:39.781263    7550 daemoncontroller.go:257] No daemon sets found for pod monitoring-influxdb-grafana-v3-n3odm, daemon set controller will avoid syncing
I1223 12:09:39.781269    7550 daemoncontroller.go:305] Pod php-apache-tx43w added.
I1223 12:09:39.781277    7550 daemoncontroller.go:257] No daemon sets found for pod php-apache-tx43w, daemon set controller will avoid syncing
I1223 12:09:39.781280    7550 daemoncontroller.go:305] Pod sample-deployment-3552496463-0tevs added.
I1223 12:09:39.781288    7550 daemoncontroller.go:257] No daemon sets found for pod sample-deployment-3552496463-0tevs, daemon set controller will avoid syncing
I1223 12:09:39.781293    7550 daemoncontroller.go:305] Pod busybox added.
I1223 12:09:39.781299    7550 daemoncontroller.go:257] No daemon sets found for pod busybox, daemon set controller will avoid syncing
I1223 12:09:39.781302    7550 daemoncontroller.go:305] Pod heapster-v1.2.0-uobac added.
I1223 12:09:39.781310    7550 daemoncontroller.go:257] No daemon sets found for pod heapster-v1.2.0-uobac, daemon set controller will avoid syncing
I1223 12:09:39.781343    7550 jobcontroller.go:141] No jobs found for pod kube-dns-v20-xojuw, job controller will avoid syncing
I1223 12:09:39.781351    7550 jobcontroller.go:141] No jobs found for pod monitoring-influxdb-grafana-v3-n3odm, job controller will avoid syncing
I1223 12:09:39.781358    7550 jobcontroller.go:141] No jobs found for pod php-apache-tx43w, job controller will avoid syncing
I1223 12:09:39.781363    7550 jobcontroller.go:141] No jobs found for pod sample-deployment-3552496463-0tevs, job controller will avoid syncing
I1223 12:09:39.781369    7550 jobcontroller.go:141] No jobs found for pod busybox, job controller will avoid syncing
I1223 12:09:39.781374    7550 jobcontroller.go:141] No jobs found for pod heapster-v1.2.0-uobac, job controller will avoid syncing
I1223 12:09:39.781382    7550 replica_set.go:288] Pod kube-dns-v20-xojuw created: &api.Pod{TypeMeta:unversioned.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:api.ObjectMeta{Name:"kube-dns-v20-xojuw", GenerateName:"kube-dns-v20-", Namespace:"kube-system", SelfLink:"/api/v1/namespaces/kube-system/pods/kube-dns-v20-xojuw", UID:"3490d291-c8de-11e6-b98e-08002750cdfb", ResourceVersion:"8048", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63618073537, nsec:0, loc:(*time.Location)(0x5cacc20)}}, DeletionTimestamp:(*unversioned.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"k8s-app":"kube-dns", "version":"v20"}, Annotations:map[string]string{"scheduler.alpha.kubernetes.io/critical-pod":"", "scheduler.alpha.kubernetes.io/tolerations":"[{\"key\":\"CriticalAddonsOnly\", \"operator\":\"Exists\"}]", "kubernetes.io/created-by":"{\"kind\":\"SerializedReference\",\"apiVersion\":\"v1\",\"reference\":{\"kind\":\"ReplicationController\",\"namespace\":\"kube-system\",\"name\":\"kube-dns-v20\",\"uid\":\"348ef075-c8de-11e6-b98e-08002750cdfb\",\"apiVersion\":\"v1\",\"resourceVersion\":\"1025\"}}\n"}, OwnerReferences:[]api.OwnerReference{api.OwnerReference{APIVersion:"v1", Kind:"ReplicationController", Name:"kube-dns-v20", UID:"348ef075-c8de-11e6-b98e-08002750cdfb", Controller:(*bool)(0xc42157f6fc)}}, Finalizers:[]string(nil), ClusterName:""}, Spec:api.PodSpec{Volumes:[]api.Volume{api.Volume{Name:"default-token-k9ean", VolumeSource:api.VolumeSource{HostPath:(*api.HostPathVolumeSource)(nil), EmptyDir:(*api.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*api.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*api.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*api.GitRepoVolumeSource)(nil), Secret:(*api.SecretVolumeSource)(0xc4215a1d70), NFS:(*api.NFSVolumeSource)(nil), ISCSI:(*api.ISCSIVolumeSource)(nil), Glusterfs:(*api.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*api.PersistentVolumeClaimVolumeSource)(nil), RBD:(*api.RBDVolumeSource)(nil), Quobyte:(*api.QuobyteVolumeSource)(nil), FlexVolume:(*api.FlexVolumeSource)(nil), Cinder:(*api.CinderVolumeSource)(nil), CephFS:(*api.CephFSVolumeSource)(nil), Flocker:(*api.FlockerVolumeSource)(nil), DownwardAPI:(*api.DownwardAPIVolumeSource)(nil), FC:(*api.FCVolumeSource)(nil), AzureFile:(*api.AzureFileVolumeSource)(nil), ConfigMap:(*api.ConfigMapVolumeSource)(nil), VsphereVolume:(*api.VsphereVirtualDiskVolumeSource)(nil), AzureDisk:(*api.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*api.PhotonPersistentDiskVolumeSource)(nil)}}}, InitContainers:[]api.Container(nil), Containers:[]api.Container{api.Container{Name:"kubedns", Image:"10.213.42.254:10500/root/kubedns-amd64:1.7", Command:[]string(nil), Args:[]string{"--domain=k8s.wanda.com.", "--dns-port=10053", "--kube_master_url=http://10.15.137.240:8080"}, WorkingDir:"", Ports:[]api.ContainerPort{api.ContainerPort{Name:"dns-local", HostPort:0, ContainerPort:10053, Protocol:"UDP", HostIP:""}, api.ContainerPort{Name:"dns-tcp-local", HostPort:0, ContainerPort:10053, Protocol:"TCP", HostIP:""}}, Env:[]api.EnvVar(nil), Resources:api.ResourceRequirements{Limits:api.ResourceList{"memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}, Requests:api.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}}, VolumeMounts:[]api.VolumeMount{api.VolumeMount{Name:"default-token-k9ean", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:""}}, LivenessProbe:(*api.Probe)(0xc4215a1e90), ReadinessProbe:(*api.Probe)(0xc4215a1ec0), Lifecycle:(*api.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", ImagePullPolicy:"IfNotPresent", SecurityContext:(*api.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, api.Container{Name:"dnsmasq", Image:"10.213.42.254:10500/root/kube-dnsmasq-amd64:1.3", Command:[]string(nil), Args:[]string{"--cache-size=1000", "--no-resolv", "--server=127.0.0.1#10053", "--log-facility=-"}, WorkingDir:"", Ports:[]api.ContainerPort{api.ContainerPort{Name:"dns", HostPort:0, ContainerPort:53, Protocol:"UDP", HostIP:""}, api.ContainerPort{Name:"dns-tcp", HostPort:0, ContainerPort:53, Protocol:"TCP", HostIP:""}}, Env:[]api.EnvVar(nil), Resources:api.ResourceRequirements{Limits:api.ResourceList(nil), Requests:api.ResourceList(nil)}, VolumeMounts:[]api.VolumeMount{api.VolumeMount{Name:"default-token-k9ean", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:""}}, LivenessProbe:(*api.Probe)(0xc4215a1f50), ReadinessProbe:(*api.Probe)(nil), Lifecycle:(*api.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", ImagePullPolicy:"IfNotPresent", SecurityContext:(*api.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, api.Container{Name:"healthz", Image:"10.213.42.254:10500/root/exechealthz-amd64:1.1", Command:[]string(nil), Args:[]string{"--cmd=nslookup kubernetes.default.svc.k8s.wanda.com 127.0.0.1 >/dev/null && nslookup kubernetes.default.svc.k8s.wanda.com 127.0.0.1:10053 >/dev/null", "--port=8080", "--quiet"}, WorkingDir:"", Ports:[]api.ContainerPort{api.ContainerPort{Name:"", HostPort:0, ContainerPort:8080, Protocol:"TCP", HostIP:""}}, Env:[]api.EnvVar(nil), Resources:api.ResourceRequirements{Limits:api.ResourceList{"memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"50Mi", Format:"BinarySI"}}, Requests:api.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:10, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"10m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"50Mi", Format:"BinarySI"}}}, VolumeMounts:[]api.VolumeMount{api.VolumeMount{Name:"default-token-k9ean", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:""}}, LivenessProbe:(*api.Probe)(nil), ReadinessProbe:(*api.Probe)(nil), Lifecycle:(*api.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", ImagePullPolicy:"IfNotPresent", SecurityContext:(*api.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc42157f888), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"Default", NodeSelector:map[string]string(nil), ServiceAccountName:"default", NodeName:"10.15.137.241", SecurityContext:(*api.PodSecurityContext)(0xc42161c5c0), ImagePullSecrets:[]api.LocalObjectReference(nil), Hostname:"", Subdomain:""}, Status:api.PodStatus{Phase:"Running", Conditions:[]api.PodCondition{api.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:unversioned.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, LastTransitionTime:unversioned.Time{Time:time.Time{sec:63618073537, nsec:0, loc:(*time.Location)(0x5cacc20)}}, Reason:"", Message:""}, api.PodCondition{Type:"Ready", Status:"True", LastProbeTime:unversioned.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, LastTransitionTime:unversioned.Time{Time:time.Time{sec:63618077360, nsec:0, loc:(*time.Location)(0x5cacc20)}}, Reason:"", Message:""}, api.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:unversioned.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, LastTransitionTime:unversioned.Time{Time:time.Time{sec:63618073537, nsec:0, loc:(*time.Location)(0x5cacc20)}}, Reason:"", Message:""}}, Message:"", Reason:"", HostIP:"10.15.137.241", PodIP:"172.17.101.2", StartTime:(*unversioned.Time)(0xc4215cb420), InitContainerStatuses:[]api.ContainerStatus(nil), ContainerStatuses:[]api.ContainerStatus{api.ContainerStatus{Name:"dnsmasq", State:api.ContainerState{Waiting:(*api.ContainerStateWaiting)(nil), Running:(*api.ContainerStateRunning)(0xc4215cb440), Terminated:(*api.ContainerStateTerminated)(nil)}, LastTerminationState:api.ContainerState{Waiting:(*api.ContainerStateWaiting)(nil), Running:(*api.ContainerStateRunning)(nil), Terminated:(*api.ContainerStateTerminated)(0xc4215d0d20)}, Ready:true, RestartCount:3, Image:"10.213.42.254:10500/root/kube-dnsmasq-amd64:1.3", ImageID:"docker://sha256:9a15e39d0db8bd3aab67c49cf198d9062b655e3e7c2d8bd0b8adf92c4a6568e8", ContainerID:"docker://914b9e994b78c01f2807caf585429802cdbbc5e1cc9211705c66e252980ae529"}, api.ContainerStatus{Name:"healthz", State:api.ContainerState{Waiting:(*api.ContainerStateWaiting)(nil), Running:(*api.ContainerStateRunning)(0xc4215cb460), Terminated:(*api.ContainerStateTerminated)(nil)}, LastTerminationState:api.ContainerState{Waiting:(*api.ContainerStateWaiting)(0xc4215cb480), Running:(*api.ContainerStateRunning)(nil), Terminated:(*api.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:4, Image:"10.213.42.254:10500/root/exechealthz-amd64:1.1", ImageID:"docker://sha256:c3a89c92ef5b7f3dbd453a590d60d3ab486a6dfc815a8c95ce92d8dfd093feca", ContainerID:"docker://7798baab77bb77692f5ccd793c3e6a0864cddacd276560674b553b6dd8f99d76"}, api.ContainerStatus{Name:"kubedns", State:api.ContainerState{Waiting:(*api.ContainerStateWaiting)(nil), Running:(*api.ContainerStateRunning)(0xc4215cb4a0), Terminated:(*api.ContainerStateTerminated)(nil)}, LastTerminationState:api.ContainerState{Waiting:(*api.ContainerStateWaiting)(nil), Running:(*api.ContainerStateRunning)(nil), Terminated:(*api.ContainerStateTerminated)(0xc4215d0d90)}, Ready:true, RestartCount:3, Image:"10.213.42.254:10500/root/kubedns-amd64:1.7", ImageID:"docker://sha256:bec33bc01f037e7c0e4aba1774996579dbe7c786001df522976c2bbce98606c8", ContainerID:"docker://f5ec5c5223af5a551ecbda4a85e64f0c50cce8b903f2fe3f586276a211eb0e37"}}}}.
I1223 12:09:39.781689    7550 replica_set.go:196] No ReplicaSets found for pod kube-dns-v20-xojuw, ReplicaSet controller will avoid syncing
I1223 12:09:39.781695    7550 replica_set.go:288] Pod monitoring-influxdb-grafana-v3-n3odm created: &api.Pod{TypeMeta:unversioned.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:api.ObjectMeta{Name:"monitoring-influxdb-grafana-v3-n3odm", GenerateName:"monitoring-influxdb-grafana-v3-", Namespace:"kube-system", SelfLink:"/api/v1/namespaces/kube-system/pods/monitoring-influxdb-grafana-v3-n3odm", UID:"d85d15cc-c8e5-11e6-85dd-08002750cdfb", ResourceVersion:"8035", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63618076818, nsec:0, loc:(*time.Location)(0x5cacc20)}}, DeletionTimestamp:(*unversioned.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"k8s-app":"influxGrafana", "kubernetes.io/cluster-service":"true", "version":"v3"}, Annotations:map[string]string{"kubernetes.io/created-by":"{\"kind\":\"SerializedReference\",\"apiVersion\":\"v1\",\"reference\":{\"kind\":\"ReplicationController\",\"namespace\":\"kube-system\",\"name\":\"monitoring-influxdb-grafana-v3\",\"uid\":\"d85c1a29-c8e5-11e6-85dd-08002750cdfb\",\"apiVersion\":\"v1\",\"resourceVersion\":\"7208\"}}\n"}, OwnerReferences:[]api.OwnerReference{api.OwnerReference{APIVersion:"v1", Kind:"ReplicationController", Name:"monitoring-influxdb-grafana-v3", UID:"d85c1a29-c8e5-11e6-85dd-08002750cdfb", Controller:(*bool)(0xc42157f96c)}}, Finalizers:[]string(nil), ClusterName:""}, Spec:api.PodSpec{Volumes:[]api.Volume{api.Volume{Name:"influxdb-persistent-storage", VolumeSource:api.VolumeSource{HostPath:(*api.HostPathVolumeSource)(nil), EmptyDir:(*api.EmptyDirVolumeSource)(0xc42157f970), GCEPersistentDisk:(*api.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*api.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*api.GitRepoVolumeSource)(nil), Secret:(*api.SecretVolumeSource)(nil), NFS:(*api.NFSVolumeSource)(nil), ISCSI:(*api.ISCSIVolumeSource)(nil), Glusterfs:(*api.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*api.PersistentVolumeClaimVolumeSource)(nil), RBD:(*api.RBDVolumeSource)(nil), Quobyte:(*api.QuobyteVolumeSource)(nil), FlexVolume:(*api.FlexVolumeSource)(nil), Cinder:(*api.CinderVolumeSource)(nil), CephFS:(*api.CephFSVolumeSource)(nil), Flocker:(*api.FlockerVolumeSource)(nil), DownwardAPI:(*api.DownwardAPIVolumeSource)(nil), FC:(*api.FCVolumeSource)(nil), AzureFile:(*api.AzureFileVolumeSource)(nil), ConfigMap:(*api.ConfigMapVolumeSource)(nil), VsphereVolume:(*api.VsphereVirtualDiskVolumeSource)(nil), AzureDisk:(*api.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*api.PhotonPersistentDiskVolumeSource)(nil)}}, api.Volume{Name:"grafana-persistent-storage", VolumeSource:api.VolumeSource{HostPath:(*api.HostPathVolumeSource)(nil), EmptyDir:(*api.EmptyDirVolumeSource)(0xc42157f980), GCEPersistentDisk:(*api.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*api.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*api.GitRepoVolumeSource)(nil), Secret:(*api.SecretVolumeSource)(nil), NFS:(*api.NFSVolumeSource)(nil), ISCSI:(*api.ISCSIVolumeSource)(nil), Glusterfs:(*api.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*api.PersistentVolumeClaimVolumeSource)(nil), RBD:(*api.RBDVolumeSource)(nil), Quobyte:(*api.QuobyteVolumeSource)(nil), FlexVolume:(*api.FlexVolumeSource)(nil), Cinder:(*api.CinderVolumeSource)(nil), CephFS:(*api.CephFSVolumeSource)(nil), Flocker:(*api.FlockerVolumeSource)(nil), DownwardAPI:(*api.DownwardAPIVolumeSource)(nil), FC:(*api.FCVolumeSource)(nil), AzureFile:(*api.AzureFileVolumeSource)(nil), ConfigMap:(*api.ConfigMapVolumeSource)(nil), VsphereVolume:(*api.VsphereVirtualDiskVolumeSource)(nil), AzureDisk:(*api.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*api.PhotonPersistentDiskVolumeSource)(nil)}}, api.Volume{Name:"default-token-k9ean", VolumeSource:api.VolumeSource{HostPath:(*api.HostPathVolumeSource)(nil), EmptyDir:(*api.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*api.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*api.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*api.GitRepoVolumeSource)(nil), Secret:(*api.SecretVolumeSource)(0xc4216141e0), NFS:(*api.NFSVolumeSource)(nil), ISCSI:(*api.ISCSIVolumeSource)(nil), Glusterfs:(*api.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*api.PersistentVolumeClaimVolumeSource)(nil), RBD:(*api.RBDVolumeSource)(nil), Quobyte:(*api.QuobyteVolumeSource)(nil), FlexVolume:(*api.FlexVolumeSource)(nil), Cinder:(*api.CinderVolumeSource)(nil), CephFS:(*api.CephFSVolumeSource)(nil), Flocker:(*api.FlockerVolumeSource)(nil), DownwardAPI:(*api.DownwardAPIVolumeSource)(nil), FC:(*api.FCVolumeSource)(nil), AzureFile:(*api.AzureFileVolumeSource)(nil), ConfigMap:(*api.ConfigMapVolumeSource)(nil), VsphereVolume:(*api.VsphereVirtualDiskVolumeSource)(nil), AzureDisk:(*api.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*api.PhotonPersistentDiskVolumeSource)(nil)}}}, InitContainers:[]api.Container(nil), Containers:[]api.Container{api.Container{Name:"influxdb", Image:"10.213.42.254:10500/google_containers/heapster_influxdb:v0.7", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]api.ContainerPort{api.ContainerPort{Name:"", HostPort:8083, ContainerPort:8083, Protocol:"TCP", HostIP:""}, api.ContainerPort{Name:"", HostPort:8086, ContainerPort:8086, Protocol:"TCP", HostIP:""}}, Env:[]api.EnvVar(nil), Resources:api.ResourceRequirements{Limits:api.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:300, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"300m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}, Requests:api.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:300, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"300m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}}, VolumeMounts:[]api.VolumeMount{api.VolumeMount{Name:"influxdb-persistent-storage", ReadOnly:false, MountPath:"/data", SubPath:""}, api.VolumeMount{Name:"default-token-k9ean", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:""}}, LivenessProbe:(*api.Probe)(nil), ReadinessProbe:(*api.Probe)(nil), Lifecycle:(*api.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", ImagePullPolicy:"IfNotPresent", SecurityContext:(*api.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, api.Container{Name:"grafana", Image:"10.213.42.254:10500/google_containers/heapster_grafana:v3.1.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]api.ContainerPort(nil), Env:[]api.EnvVar(nil), Resources:api.ResourceRequirements{Limits:api.ResourceList{"memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}, "cpu":resource.Quantity{i:resource.int64Amount{value:300, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"300m", Format:"DecimalSI"}}, Requests:api.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:300, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"300m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}}, VolumeMounts:[]api.VolumeMount{api.VolumeMount{Name:"grafana-persistent-storage", ReadOnly:false, MountPath:"/var", SubPath:""}, api.VolumeMount{Name:"default-token-k9ean", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:""}}, LivenessProbe:(*api.Probe)(nil), ReadinessProbe:(*api.Probe)(nil), Lifecycle:(*api.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", ImagePullPolicy:"IfNotPresent", SecurityContext:(*api.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc42157fa38), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", NodeName:"10.15.137.241", SecurityContext:(*api.PodSecurityContext)(0xc42161c600), ImagePullSecrets:[]api.LocalObjectReference(nil), Hostname:"", Subdomain:""}, Status:api.PodStatus{Phase:"Running", Conditions:[]api.PodCondition{api.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:unversioned.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, LastTransitionTime:unversioned.Time{Time:time.Time{sec:63618076818, nsec:0, loc:(*time.Location)(0x5cacc20)}}, Reason:"", Message:""}, api.PodCondition{Type:"Ready", Status:"True", LastProbeTime:unversioned.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, LastTransitionTime:unversioned.Time{Time:time.Time{sec:63618077354, nsec:0, loc:(*time.Location)(0x5cacc20)}}, Reason:"", Message:""}, api.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:unversioned.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, LastTransitionTime:unversioned.Time{Time:time.Time{sec:63618076818, nsec:0, loc:(*time.Location)(0x5cacc20)}}, Reason:"", Message:""}}, Message:"", Reason:"", HostIP:"10.15.137.241", PodIP:"172.17.101.3", StartTime:(*unversioned.Time)(0xc4215cb6a0), InitContainerStatuses:[]api.ContainerStatus(nil), ContainerStatuses:[]api.ContainerStatus{api.ContainerStatus{Name:"grafana", State:api.ContainerState{Waiting:(*api.ContainerStateWaiting)(nil), Running:(*api.ContainerStateRunning)(0xc4215cb6c0), Terminated:(*api.ContainerStateTerminated)(nil)}, LastTerminationState:api.ContainerState{Waiting:(*api.ContainerStateWaiting)(nil), Running:(*api.ContainerStateRunning)(nil), Terminated:(*api.ContainerStateTerminated)(0xc4215d0fc0)}, Ready:true, RestartCount:1, Image:"10.213.42.254:10500/google_containers/heapster_grafana:v3.1.1", ImageID:"docker://sha256:41b92a01197f7605d5805f8a7bbebf8f4972c635662e701dbedb03bc58c71412", ContainerID:"docker://96fa9590b1223f0a6154456d68cb8354dd9930e2ce2bce528b5e6da21bf23964"}, api.ContainerStatus{Name:"influxdb", State:api.ContainerState{Waiting:(*api.ContainerStateWaiting)(nil), Running:(*api.ContainerStateRunning)(0xc4215cb6e0), Terminated:(*api.ContainerStateTerminated)(nil)}, LastTerminationState:api.ContainerState{Waiting:(*api.ContainerStateWaiting)(nil), Running:(*api.ContainerStateRunning)(nil), Terminated:(*api.ContainerStateTerminated)(0xc4215d1030)}, Ready:true, RestartCount:1, Image:"10.213.42.254:10500/google_containers/heapster_influxdb:v0.7", ImageID:"docker://sha256:b9a29a864750afa03dc00fc17f3ddaed026793fec104af2ad12b9fb4e2f889f1", ContainerID:"docker://61f9efb0b36b2f2d119f096b24ba6a74df3fd42fe5ba4403ea4449d7f81c1801"}}}}.
I1223 12:09:39.781859    7550 replica_set.go:196] No ReplicaSets found for pod monitoring-influxdb-grafana-v3-n3odm, ReplicaSet controller will avoid syncing
I1223 12:09:39.781866    7550 replica_set.go:288] Pod php-apache-tx43w created: &api.Pod{TypeMeta:unversioned.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:api.ObjectMeta{Name:"php-apache-tx43w", GenerateName:"php-apache-", Namespace:"kube-system", SelfLink:"/api/v1/namespaces/kube-system/pods/php-apache-tx43w", UID:"eb9fb486-c8e5-11e6-85dd-08002750cdfb", ResourceVersion:"7926", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63618076850, nsec:0, loc:(*time.Location)(0x5cacc20)}}, DeletionTimestamp:(*unversioned.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"php-apache"}, Annotations:map[string]string{"kubernetes.io/created-by":"{\"kind\":\"SerializedReference\",\"apiVersion\":\"v1\",\"reference\":{\"kind\":\"ReplicationController\",\"namespace\":\"kube-system\",\"name\":\"php-apache\",\"uid\":\"eb9f0081-c8e5-11e6-85dd-08002750cdfb\",\"apiVersion\":\"v1\",\"resourceVersion\":\"7299\"}}\n"}, OwnerReferences:[]api.OwnerReference{api.OwnerReference{APIVersion:"v1", Kind:"ReplicationController", Name:"php-apache", UID:"eb9f0081-c8e5-11e6-85dd-08002750cdfb", Controller:(*bool)(0xc42157fb3c)}}, Finalizers:[]string(nil), ClusterName:""}, Spec:api.PodSpec{Volumes:[]api.Volume{api.Volume{Name:"default-token-k9ean", VolumeSource:api.VolumeSource{HostPath:(*api.HostPathVolumeSource)(nil), EmptyDir:(*api.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*api.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*api.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*api.GitRepoVolumeSource)(nil), Secret:(*api.SecretVolumeSource)(0xc4216143f0), NFS:(*api.NFSVolumeSource)(nil), ISCSI:(*api.ISCSIVolumeSource)(nil), Glusterfs:(*api.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*api.PersistentVolumeClaimVolumeSource)(nil), RBD:(*api.RBDVolumeSource)(nil), Quobyte:(*api.QuobyteVolumeSource)(nil), FlexVolume:(*api.FlexVolumeSource)(nil), Cinder:(*api.CinderVolumeSource)(nil), CephFS:(*api.CephFSVolumeSource)(nil), Flocker:(*api.FlockerVolumeSource)(nil), DownwardAPI:(*api.DownwardAPIVolumeSource)(nil), FC:(*api.FCVolumeSource)(nil), AzureFile:(*api.AzureFileVolumeSource)(nil), ConfigMap:(*api.ConfigMapVolumeSource)(nil), VsphereVolume:(*api.VsphereVirtualDiskVolumeSource)(nil), AzureDisk:(*api.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*api.PhotonPersistentDiskVolumeSource)(nil)}}}, InitContainers:[]api.Container(nil), Containers:[]api.Container{api.Container{Name:"php-apache", Image:"10.213.42.254:10500/caozhiqiang1/hpa-example:2.5", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]api.ContainerPort{api.ContainerPort{Name:"", HostPort:8500, ContainerPort:80, Protocol:"TCP", HostIP:""}}, Env:[]api.EnvVar(nil), Resources:api.ResourceRequirements{Limits:api.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:209715200, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"", Format:"BinarySI"}}, Requests:api.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:209715200, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"", Format:"BinarySI"}}}, VolumeMounts:[]api.VolumeMount{api.VolumeMount{Name:"default-token-k9ean", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:""}}, LivenessProbe:(*api.Probe)(nil), ReadinessProbe:(*api.Probe)(nil), Lifecycle:(*api.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", ImagePullPolicy:"IfNotPresent", SecurityContext:(*api.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc42157fba8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", NodeName:"10.15.137.242", SecurityContext:(*api.PodSecurityContext)(0xc42161c640), ImagePullSecrets:[]api.LocalObjectReference(nil), Hostname:"", Subdomain:""}, Status:api.PodStatus{Phase:"Running", Conditions:[]api.PodCondition{api.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:unversioned.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, LastTransitionTime:unversioned.Time{Time:time.Time{sec:63618076850, nsec:0, loc:(*time.Location)(0x5cacc20)}}, Reason:"", Message:""}, api.PodCondition{Type:"Ready", Status:"True", LastProbeTime:unversioned.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, LastTransitionTime:unversioned.Time{Time:time.Time{sec:63618077300, nsec:0, loc:(*time.Location)(0x5cacc20)}}, Reason:"", Message:""}, api.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:unversioned.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, LastTransitionTime:unversioned.Time{Time:time.Time{sec:63618076850, nsec:0, loc:(*time.Location)(0x5cacc20)}}, Reason:"", Message:""}}, Message:"", Reason:"", HostIP:"10.15.137.242", PodIP:"172.17.17.4", StartTime:(*unversioned.Time)(0xc4215cb7c0), InitContainerStatuses:[]api.ContainerStatus(nil), ContainerStatuses:[]api.ContainerStatus{api.ContainerStatus{Name:"php-apache", State:api.ContainerState{Waiting:(*api.ContainerStateWaiting)(nil), Running:(*api.ContainerStateRunning)(0xc4215cb7e0), Terminated:(*api.ContainerStateTerminated)(nil)}, LastTerminationState:api.ContainerState{Waiting:(*api.ContainerStateWaiting)(0xc4215cb800), Running:(*api.ContainerStateRunning)(nil), Terminated:(*api.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:2, Image:"10.213.42.254:10500/caozhiqiang1/hpa-example:2.5", ImageID:"docker://sha256:475b01cb5493f0f75f7f9f2c8b71d2b0036072f8bea3bc0cb77082c53f700628", ContainerID:"docker://8bc835bb1e0785018725db262010fc57c02634871f0a4905e02c99c05a65be34"}}}}.
I1223 12:09:39.781968    7550 replica_set.go:196] No ReplicaSets found for pod php-apache-tx43w, ReplicaSet controller will avoid syncing
I1223 12:09:39.781974    7550 replica_set.go:288] Pod sample-deployment-3552496463-0tevs created: &api.Pod{TypeMeta:unversioned.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:api.ObjectMeta{Name:"sample-deployment-3552496463-0tevs", GenerateName:"sample-deployment-3552496463-", Namespace:"default", SelfLink:"/api/v1/namespaces/default/pods/sample-deployment-3552496463-0tevs", UID:"e074fc94-c8e5-11e6-85dd-08002750cdfb", ResourceVersion:"7908", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63618076832, nsec:0, loc:(*time.Location)(0x5cacc20)}}, DeletionTimestamp:(*unversioned.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"sample-deployment", "pod-template-hash":"3552496463"}, Annotations:map[string]string{"kubernetes.io/created-by":"{\"kind\":\"SerializedReference\",\"apiVersion\":\"v1\",\"reference\":{\"kind\":\"ReplicaSet\",\"namespace\":\"default\",\"name\":\"sample-deployment-3552496463\",\"uid\":\"e070fa5b-c8e5-11e6-85dd-08002750cdfb\",\"apiVersion\":\"extensions\",\"resourceVersion\":\"7252\"}}\n"}, OwnerReferences:[]api.OwnerReference{api.OwnerReference{APIVersion:"extensions/v1beta1", Kind:"ReplicaSet", Name:"sample-deployment-3552496463", UID:"e070fa5b-c8e5-11e6-85dd-08002750cdfb", Controller:(*bool)(0xc42157fc6a)}}, Finalizers:[]string(nil), ClusterName:""}, Spec:api.PodSpec{Volumes:[]api.Volume{api.Volume{Name:"default-token-43qlq", VolumeSource:api.VolumeSource{HostPath:(*api.HostPathVolumeSource)(nil), EmptyDir:(*api.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*api.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*api.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*api.GitRepoVolumeSource)(nil), Secret:(*api.SecretVolumeSource)(0xc421614600), NFS:(*api.NFSVolumeSource)(nil), ISCSI:(*api.ISCSIVolumeSource)(nil), Glusterfs:(*api.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*api.PersistentVolumeClaimVolumeSource)(nil), RBD:(*api.RBDVolumeSource)(nil), Quobyte:(*api.QuobyteVolumeSource)(nil), FlexVolume:(*api.FlexVolumeSource)(nil), Cinder:(*api.CinderVolumeSource)(nil), CephFS:(*api.CephFSVolumeSource)(nil), Flocker:(*api.FlockerVolumeSource)(nil), DownwardAPI:(*api.DownwardAPIVolumeSource)(nil), FC:(*api.FCVolumeSource)(nil), AzureFile:(*api.AzureFileVolumeSource)(nil), ConfigMap:(*api.ConfigMapVolumeSource)(nil), VsphereVolume:(*api.VsphereVirtualDiskVolumeSource)(nil), AzureDisk:(*api.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*api.PhotonPersistentDiskVolumeSource)(nil)}}}, InitContainers:[]api.Container(nil), Containers:[]api.Container{api.Container{Name:"sample", Image:"10.213.42.254:10500/huangyujun6/sample:1.0", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]api.ContainerPort{api.ContainerPort{Name:"", HostPort:0, ContainerPort:8080, Protocol:"TCP", HostIP:""}}, Env:[]api.EnvVar(nil), Resources:api.ResourceRequirements{Limits:api.ResourceList(nil), Requests:api.ResourceList(nil)}, VolumeMounts:[]api.VolumeMount{api.VolumeMount{Name:"default-token-43qlq", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:""}}, LivenessProbe:(*api.Probe)(nil), ReadinessProbe:(*api.Probe)(nil), Lifecycle:(*api.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", ImagePullPolicy:"IfNotPresent", SecurityContext:(*api.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc42157fca0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", NodeName:"10.15.137.242", SecurityContext:(*api.PodSecurityContext)(0xc42161c680), ImagePullSecrets:[]api.LocalObjectReference(nil), Hostname:"", Subdomain:""}, Status:api.PodStatus{Phase:"Running", Conditions:[]api.PodCondition{api.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:unversioned.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, LastTransitionTime:unversioned.Time{Time:time.Time{sec:63618076832, nsec:0, loc:(*time.Location)(0x5cacc20)}}, Reason:"", Message:""}, api.PodCondition{Type:"Ready", Status:"True", LastProbeTime:unversioned.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, LastTransitionTime:unversioned.Time{Time:time.Time{sec:63618077298, nsec:0, loc:(*time.Location)(0x5cacc20)}}, Reason:"", Message:""}, api.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:unversioned.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, LastTransitionTime:unversioned.Time{Time:time.Time{sec:63618076832, nsec:0, loc:(*time.Location)(0x5cacc20)}}, Reason:"", Message:""}}, Message:"", Reason:"", HostIP:"10.15.137.242", PodIP:"172.17.17.3", StartTime:(*unversioned.Time)(0xc4215cb960), InitContainerStatuses:[]api.ContainerStatus(nil), ContainerStatuses:[]api.ContainerStatus{api.ContainerStatus{Name:"sample", State:api.ContainerState{Waiting:(*api.ContainerStateWaiting)(nil), Running:(*api.ContainerStateRunning)(0xc4215cb980), Terminated:(*api.ContainerStateTerminated)(nil)}, LastTerminationState:api.ContainerState{Waiting:(*api.ContainerStateWaiting)(nil), Running:(*api.ContainerStateRunning)(nil), Terminated:(*api.ContainerStateTerminated)(0xc4215d1180)}, Ready:true, RestartCount:1, Image:"10.213.42.254:10500/huangyujun6/sample:1.0", ImageID:"docker://sha256:425ee31ad71db4fde1f85dcecbd49289d185e71d1d2ab70d6f5b97adb73b81b0", ContainerID:"docker://3a1dbab8a982402c33a650b3da3fa82b2d1cccb7ebdb833b317da65dc803112d"}}}}.
I1223 12:09:39.782109    7550 replica_set.go:288] Pod busybox created: &api.Pod{TypeMeta:unversioned.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:api.ObjectMeta{Name:"busybox", GenerateName:"", Namespace:"kube-system", SelfLink:"/api/v1/namespaces/kube-system/pods/busybox", UID:"f427d5c2-c8e5-11e6-85dd-08002750cdfb", ResourceVersion:"26249", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63618076865, nsec:0, loc:(*time.Location)(0x5cacc20)}}, DeletionTimestamp:(*unversioned.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]api.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:api.PodSpec{Volumes:[]api.Volume{api.Volume{Name:"default-token-k9ean", VolumeSource:api.VolumeSource{HostPath:(*api.HostPathVolumeSource)(nil), EmptyDir:(*api.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*api.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*api.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*api.GitRepoVolumeSource)(nil), Secret:(*api.SecretVolumeSource)(0xc421614720), NFS:(*api.NFSVolumeSource)(nil), ISCSI:(*api.ISCSIVolumeSource)(nil), Glusterfs:(*api.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*api.PersistentVolumeClaimVolumeSource)(nil), RBD:(*api.RBDVolumeSource)(nil), Quobyte:(*api.QuobyteVolumeSource)(nil), FlexVolume:(*api.FlexVolumeSource)(nil), Cinder:(*api.CinderVolumeSource)(nil), CephFS:(*api.CephFSVolumeSource)(nil), Flocker:(*api.FlockerVolumeSource)(nil), DownwardAPI:(*api.DownwardAPIVolumeSource)(nil), FC:(*api.FCVolumeSource)(nil), AzureFile:(*api.AzureFileVolumeSource)(nil), ConfigMap:(*api.ConfigMapVolumeSource)(nil), VsphereVolume:(*api.VsphereVirtualDiskVolumeSource)(nil), AzureDisk:(*api.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*api.PhotonPersistentDiskVolumeSource)(nil)}}}, InitContainers:[]api.Container(nil), Containers:[]api.Container{api.Container{Name:"busybox", Image:"10.213.42.254:10500/caozhiqiang1/busybox", Command:[]string{"sleep", "3600"}, Args:[]string(nil), WorkingDir:"", Ports:[]api.ContainerPort(nil), Env:[]api.EnvVar(nil), Resources:api.ResourceRequirements{Limits:api.ResourceList(nil), Requests:api.ResourceList(nil)}, VolumeMounts:[]api.VolumeMount{api.VolumeMount{Name:"default-token-k9ean", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:""}}, LivenessProbe:(*api.Probe)(nil), ReadinessProbe:(*api.Probe)(nil), Lifecycle:(*api.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", ImagePullPolicy:"IfNotPresent", SecurityContext:(*api.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc42157fd98), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", NodeName:"10.15.137.242", SecurityContext:(*api.PodSecurityContext)(0xc42161c6c0), ImagePullSecrets:[]api.LocalObjectReference(nil), Hostname:"", Subdomain:""}, Status:api.PodStatus{Phase:"Running", Conditions:[]api.PodCondition{api.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:unversioned.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, LastTransitionTime:unversioned.Time{Time:time.Time{sec:63618076865, nsec:0, loc:(*time.Location)(0x5cacc20)}}, Reason:"", Message:""}, api.PodCondition{Type:"Ready", Status:"False", LastProbeTime:unversioned.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, LastTransitionTime:unversioned.Time{Time:time.Time{sec:63618091754, nsec:0, loc:(*time.Location)(0x5cacc20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [busybox]"}, api.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:unversioned.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, LastTransitionTime:unversioned.Time{Time:time.Time{sec:63618076865, nsec:0, loc:(*time.Location)(0x5cacc20)}}, Reason:"", Message:""}}, Message:"", Reason:"", HostIP:"10.15.137.242", PodIP:"172.17.17.2", StartTime:(*unversioned.Time)(0xc4215cba60), InitContainerStatuses:[]api.ContainerStatus(nil), ContainerStatuses:[]api.ContainerStatus{api.ContainerStatus{Name:"busybox", State:api.ContainerState{Waiting:(*api.ContainerStateWaiting)(nil), Running:(*api.ContainerStateRunning)(nil), Terminated:(*api.ContainerStateTerminated)(0xc4215d1260)}, LastTerminationState:api.ContainerState{Waiting:(*api.ContainerStateWaiting)(nil), Running:(*api.ContainerStateRunning)(nil), Terminated:(*api.ContainerStateTerminated)(0xc4215d12d0)}, Ready:false, RestartCount:4, Image:"10.213.42.254:10500/caozhiqiang1/busybox", ImageID:"docker://sha256:e02e811dd08fd49e7f6032625495118e63f597eb150403d02e3238af1df240ba", ContainerID:"docker://2fa6f99c5064d861f1043733ffe5c4531ffaa7c2a3c9531528493f58cc1edb7e"}}}}.
I1223 12:09:39.782182    7550 replica_set.go:196] No ReplicaSets found for pod busybox, ReplicaSet controller will avoid syncing
I1223 12:09:39.782186    7550 replica_set.go:288] Pod heapster-v1.2.0-uobac created: &api.Pod{TypeMeta:unversioned.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:api.ObjectMeta{Name:"heapster-v1.2.0-uobac", GenerateName:"heapster-v1.2.0-", Namespace:"kube-system", SelfLink:"/api/v1/namespaces/kube-system/pods/heapster-v1.2.0-uobac", UID:"d8500048-c8e5-11e6-85dd-08002750cdfb", ResourceVersion:"7931", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63618076818, nsec:0, loc:(*time.Location)(0x5cacc20)}}, DeletionTimestamp:(*unversioned.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"k8s-app":"heapster", "kubernetes.io/cluster-service":"true"}, Annotations:map[string]string{"kubernetes.io/created-by":"{\"kind\":\"SerializedReference\",\"apiVersion\":\"v1\",\"reference\":{\"kind\":\"ReplicationController\",\"namespace\":\"kube-system\",\"name\":\"heapster-v1.2.0\",\"uid\":\"d84f1d82-c8e5-11e6-85dd-08002750cdfb\",\"apiVersion\":\"v1\",\"resourceVersion\":\"7199\"}}\n"}, OwnerReferences:[]api.OwnerReference{api.OwnerReference{APIVersion:"v1", Kind:"ReplicationController", Name:"heapster-v1.2.0", UID:"d84f1d82-c8e5-11e6-85dd-08002750cdfb", Controller:(*bool)(0xc42157fe86)}}, Finalizers:[]string(nil), ClusterName:""}, Spec:api.PodSpec{Volumes:[]api.Volume{api.Volume{Name:"default-token-k9ean", VolumeSource:api.VolumeSource{HostPath:(*api.HostPathVolumeSource)(nil), EmptyDir:(*api.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*api.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*api.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*api.GitRepoVolumeSource)(nil), Secret:(*api.SecretVolumeSource)(0xc4216148d0), NFS:(*api.NFSVolumeSource)(nil), ISCSI:(*api.ISCSIVolumeSource)(nil), Glusterfs:(*api.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*api.PersistentVolumeClaimVolumeSource)(nil), RBD:(*api.RBDVolumeSource)(nil), Quobyte:(*api.QuobyteVolumeSource)(nil), FlexVolume:(*api.FlexVolumeSource)(nil), Cinder:(*api.CinderVolumeSource)(nil), CephFS:(*api.CephFSVolumeSource)(nil), Flocker:(*api.FlockerVolumeSource)(nil), DownwardAPI:(*api.DownwardAPIVolumeSource)(nil), FC:(*api.FCVolumeSource)(nil), AzureFile:(*api.AzureFileVolumeSource)(nil), ConfigMap:(*api.ConfigMapVolumeSource)(nil), VsphereVolume:(*api.VsphereVirtualDiskVolumeSource)(nil), AzureDisk:(*api.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*api.PhotonPersistentDiskVolumeSource)(nil)}}}, InitContainers:[]api.Container(nil), Containers:[]api.Container{api.Container{Name:"heapster", Image:"10.213.42.254:10500/google_containers/heapster:v1.2.0", Command:[]string{"/heapster", "--source=kubernetes.summary_api:http://10.15.137.240:8080?inClusterConfig=false", "--sink=influxdb:http://monitoring-influxdb:8086", "--metric_resolution=60s"}, Args:[]string(nil), WorkingDir:"", Ports:[]api.ContainerPort(nil), Env:[]api.EnvVar(nil), Resources:api.ResourceRequirements{Limits:api.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:200, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"200m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:629145600, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"", Format:"BinarySI"}}, Requests:api.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:200, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"200m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:629145600, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"", Format:"BinarySI"}}}, VolumeMounts:[]api.VolumeMount{api.VolumeMount{Name:"default-token-k9ean", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:""}}, LivenessProbe:(*api.Probe)(nil), ReadinessProbe:(*api.Probe)(nil), Lifecycle:(*api.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", ImagePullPolicy:"IfNotPresent", SecurityContext:(*api.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, api.Container{Name:"eventer", Image:"10.213.42.254:10500/google_containers/heapster:v1.2.0", Command:[]string{"/eventer", "--source=kubernetes:http://10.15.137.240:8080?inClusterConfig=false", "--sink=influxdb:http://monitoring-influxdb:8086"}, Args:[]string(nil), WorkingDir:"", Ports:[]api.ContainerPort(nil), Env:[]api.EnvVar(nil), Resources:api.ResourceRequirements{Limits:api.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:314572800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"300Mi", Format:"BinarySI"}}, Requests:api.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:314572800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"300Mi", Format:"BinarySI"}}}, VolumeMounts:[]api.VolumeMount{api.VolumeMount{Name:"default-token-k9ean", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:""}}, LivenessProbe:(*api.Probe)(nil), ReadinessProbe:(*api.Probe)(nil), Lifecycle:(*api.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", ImagePullPolicy:"IfNotPresent", SecurityContext:(*api.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc42157ff68), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", NodeName:"10.15.137.242", SecurityContext:(*api.PodSecurityContext)(0xc42161c700), ImagePullSecrets:[]api.LocalObjectReference(nil), Hostname:"", Subdomain:""}, Status:api.PodStatus{Phase:"Running", Conditions:[]api.PodCondition{api.PodCondition{Type:"Initialized", Status:"True", LastProbeTime:unversioned.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, LastTransitionTime:unversioned.Time{Time:time.Time{sec:63618076818, nsec:0, loc:(*time.Location)(0x5cacc20)}}, Reason:"", Message:""}, api.PodCondition{Type:"Ready", Status:"True", LastProbeTime:unversioned.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, LastTransitionTime:unversioned.Time{Time:time.Time{sec:63618077301, nsec:0, loc:(*time.Location)(0x5cacc20)}}, Reason:"", Message:""}, api.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:unversioned.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, LastTransitionTime:unversioned.Time{Time:time.Time{sec:63618076818, nsec:0, loc:(*time.Location)(0x5cacc20)}}, Reason:"", Message:""}}, Message:"", Reason:"", HostIP:"10.15.137.242", PodIP:"172.17.17.5", StartTime:(*unversioned.Time)(0xc4215cbc20), InitContainerStatuses:[]api.ContainerStatus(nil), ContainerStatuses:[]api.ContainerStatus{api.ContainerStatus{Name:"eventer", State:api.ContainerState{Waiting:(*api.ContainerStateWaiting)(nil), Running:(*api.ContainerStateRunning)(0xc4215cbc40), Terminated:(*api.ContainerStateTerminated)(nil)}, LastTerminationState:api.ContainerState{Waiting:(*api.ContainerStateWaiting)(nil), Running:(*api.ContainerStateRunning)(nil), Terminated:(*api.ContainerStateTerminated)(0xc4215d13b0)}, Ready:true, RestartCount:1, Image:"10.213.42.254:10500/google_containers/heapster:v1.2.0", ImageID:"docker://sha256:7cd51f2f6a9741aee93a73dfc48ee22db540a8c8d08147e0ee23ddeef10e9442", ContainerID:"docker://e7946d1d63103b51a179994fc9786071e36abca124e27e51ef5419fcc1178d1b"}, api.ContainerStatus{Name:"heapster", State:api.ContainerState{Waiting:(*api.ContainerStateWaiting)(nil), Running:(*api.ContainerStateRunning)(0xc4215cbc60), Terminated:(*api.ContainerStateTerminated)(nil)}, LastTerminationState:api.ContainerState{Waiting:(*api.ContainerStateWaiting)(0xc4215cbc80), Running:(*api.ContainerStateRunning)(nil), Terminated:(*api.ContainerStateTerminated)(nil)}, Ready:true, RestartCount:2, Image:"10.213.42.254:10500/google_containers/heapster:v1.2.0", ImageID:"docker://sha256:7cd51f2f6a9741aee93a73dfc48ee22db540a8c8d08147e0ee23ddeef10e9442", ContainerID:"docker://fc5c5e5a79a061687df71a314945be60245e0fec6db68f3481f26b2931f28854"}}}}.
I1223 12:09:39.782311    7550 replica_set.go:196] No ReplicaSets found for pod heapster-v1.2.0-uobac, ReplicaSet controller will avoid syncing
I1223 12:09:39.782323    7550 disruption.go:314] addPod called on pod "kube-dns-v20-xojuw"
I1223 12:09:39.782351    7550 disruption.go:389] No PodDisruptionBudgets found for pod kube-dns-v20-xojuw, PodDisruptionBudget controller will avoid syncing.
I1223 12:09:39.782355    7550 disruption.go:317] No matching pdb for pod "kube-dns-v20-xojuw"
I1223 12:09:39.782359    7550 disruption.go:314] addPod called on pod "monitoring-influxdb-grafana-v3-n3odm"
I1223 12:09:39.782365    7550 disruption.go:389] No PodDisruptionBudgets found for pod monitoring-influxdb-grafana-v3-n3odm, PodDisruptionBudget controller will avoid syncing.
I1223 12:09:39.782368    7550 disruption.go:317] No matching pdb for pod "monitoring-influxdb-grafana-v3-n3odm"
I1223 12:09:39.782373    7550 disruption.go:314] addPod called on pod "php-apache-tx43w"
I1223 12:09:39.782379    7550 disruption.go:389] No PodDisruptionBudgets found for pod php-apache-tx43w, PodDisruptionBudget controller will avoid syncing.
I1223 12:09:39.782382    7550 disruption.go:317] No matching pdb for pod "php-apache-tx43w"
I1223 12:09:39.782385    7550 disruption.go:314] addPod called on pod "sample-deployment-3552496463-0tevs"
I1223 12:09:39.782391    7550 disruption.go:389] No PodDisruptionBudgets found for pod sample-deployment-3552496463-0tevs, PodDisruptionBudget controller will avoid syncing.
I1223 12:09:39.782399    7550 disruption.go:317] No matching pdb for pod "sample-deployment-3552496463-0tevs"
I1223 12:09:39.782405    7550 disruption.go:314] addPod called on pod "busybox"
I1223 12:09:39.782410    7550 disruption.go:389] No PodDisruptionBudgets found for pod busybox, PodDisruptionBudget controller will avoid syncing.
I1223 12:09:39.782412    7550 disruption.go:317] No matching pdb for pod "busybox"
I1223 12:09:39.782416    7550 disruption.go:314] addPod called on pod "heapster-v1.2.0-uobac"
I1223 12:09:39.782421    7550 disruption.go:389] No PodDisruptionBudgets found for pod heapster-v1.2.0-uobac, PodDisruptionBudget controller will avoid syncing.
I1223 12:09:39.782424    7550 disruption.go:317] No matching pdb for pod "heapster-v1.2.0-uobac"
I1223 12:09:39.782432    7550 pet_set.go:160] Pod kube-dns-v20-xojuw created, labels: map[k8s-app:kube-dns version:v20]
I1223 12:09:39.782452    7550 pet_set.go:239] No StatefulSets found for pod kube-dns-v20-xojuw, StatefulSet controller will avoid syncing
I1223 12:09:39.782455    7550 pet_set.go:160] Pod monitoring-influxdb-grafana-v3-n3odm created, labels: map[version:v3 k8s-app:influxGrafana kubernetes.io/cluster-service:true]
I1223 12:09:39.782462    7550 pet_set.go:239] No StatefulSets found for pod monitoring-influxdb-grafana-v3-n3odm, StatefulSet controller will avoid syncing
I1223 12:09:39.782468    7550 pet_set.go:160] Pod php-apache-tx43w created, labels: map[app:php-apache]
I1223 12:09:39.782474    7550 pet_set.go:239] No StatefulSets found for pod php-apache-tx43w, StatefulSet controller will avoid syncing
I1223 12:09:39.782477    7550 pet_set.go:160] Pod sample-deployment-3552496463-0tevs created, labels: map[app:sample-deployment pod-template-hash:3552496463]
I1223 12:09:39.782484    7550 pet_set.go:239] No StatefulSets found for pod sample-deployment-3552496463-0tevs, StatefulSet controller will avoid syncing
I1223 12:09:39.782488    7550 pet_set.go:160] Pod busybox created, labels: map[]
I1223 12:09:39.782492    7550 pet_set.go:239] No StatefulSets found for pod busybox, StatefulSet controller will avoid syncing
I1223 12:09:39.782495    7550 pet_set.go:160] Pod heapster-v1.2.0-uobac created, labels: map[k8s-app:heapster kubernetes.io/cluster-service:true]
I1223 12:09:39.782501    7550 pet_set.go:239] No StatefulSets found for pod heapster-v1.2.0-uobac, StatefulSet controller will avoid syncing
I1223 12:09:39.811032    7550 shared_informer.go:105] caches populated
I1223 12:09:39.812226    7550 shared_informer.go:105] caches populated
I1223 12:09:39.819820    7550 endpoints_controller.go:334] Finished syncing service "kube-system/monitoring-influxdb" endpoints. (6.722269ms)
I1223 12:09:39.819863    7550 endpoints_controller.go:334] Finished syncing service "default/kubernetes" endpoints. (1.784µs)
I1223 12:09:39.820034    7550 endpoints_controller.go:334] Finished syncing service "kube-system/heapster" endpoints. (7.485478ms)
I1223 12:09:39.820437    7550 endpoints_controller.go:334] Finished syncing service "kube-system/kube-dns" endpoints. (7.666848ms)
I1223 12:09:39.820508    7550 endpoints_controller.go:334] Finished syncing service "kube-system/monitoring-grafana" endpoints. (7.564465ms)
I1223 12:09:39.820547    7550 endpoints_controller.go:334] Finished syncing service "kube-system/php-apache" endpoints. (7.297626ms)
I1223 12:09:39.820644    7550 nodecontroller.go:419] NodeController observed a new Node: "10.15.137.242"
I1223 12:09:39.820654    7550 controller_utils.go:274] Recording Registered Node 10.15.137.242 in NodeController event message for node 10.15.137.242
I1223 12:09:39.820674    7550 nodecontroller.go:429] Initializing eviction metric for zone: 
I1223 12:09:39.820682    7550 nodecontroller.go:419] NodeController observed a new Node: "10.15.137.241"
I1223 12:09:39.820686    7550 controller_utils.go:274] Recording Registered Node 10.15.137.241 in NodeController event message for node 10.15.137.241
W1223 12:09:39.820704    7550 nodecontroller.go:678] Missing timestamp for Node 10.15.137.242. Assuming now as a timestamp.
W1223 12:09:39.820753    7550 nodecontroller.go:678] Missing timestamp for Node 10.15.137.241. Assuming now as a timestamp.
I1223 12:09:39.820792    7550 nodecontroller.go:608] NodeController detected that zone  is now in state Normal.
I1223 12:09:39.820944    7550 event.go:217] Event(api.ObjectReference{Kind:"Node", Namespace:"", Name:"10.15.137.241", UID:"d5be6f5d-c8dc-11e6-b98e-08002750cdfb", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RegisteredNode' Node 10.15.137.241 event: Registered Node 10.15.137.241 in NodeController
I1223 12:09:39.820969    7550 event.go:217] Event(api.ObjectReference{Kind:"Node", Namespace:"", Name:"10.15.137.242", UID:"d80f4011-c8dc-11e6-b98e-08002750cdfb", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RegisteredNode' Node 10.15.137.242 event: Registered Node 10.15.137.242 in NodeController
I1223 12:09:39.821046    7550 replication_controller.go:653] Waiting for pods controller to sync, requeuing rc kube-system/php-apache
I1223 12:09:39.821059    7550 replication_controller.go:647] Finished syncing controller "kube-system/php-apache" (105.633903ms)
I1223 12:09:39.821084    7550 controller_utils.go:158] Controller kube-system/php-apache either never recorded expectations, or the ttl expired.
I1223 12:09:39.821101    7550 controller_ref_manager.go:79] Ignoring pod kube-system/heapster-v1.2.0-uobac, it's owned by [v1/ReplicationController, name: heapster-v1.2.0, uid: d84f1d82-c8e5-11e6-85dd-08002750cdfb]
I1223 12:09:39.821109    7550 controller_ref_manager.go:79] Ignoring pod kube-system/kube-dns-v20-xojuw, it's owned by [v1/ReplicationController, name: kube-dns-v20, uid: 348ef075-c8de-11e6-b98e-08002750cdfb]
I1223 12:09:39.821114    7550 controller_ref_manager.go:79] Ignoring pod kube-system/monitoring-influxdb-grafana-v3-n3odm, it's owned by [v1/ReplicationController, name: monitoring-influxdb-grafana-v3, uid: d85c1a29-c8e5-11e6-85dd-08002750cdfb]
I1223 12:09:39.821123    7550 replication_controller.go:647] Finished syncing controller "kube-system/php-apache" (48.703µs)
I1223 12:09:39.821134    7550 replication_controller.go:653] Waiting for pods controller to sync, requeuing rc kube-system/heapster-v1.2.0
I1223 12:09:39.821137    7550 replication_controller.go:647] Finished syncing controller "kube-system/heapster-v1.2.0" (105.82881ms)
I1223 12:09:39.821147    7550 controller_utils.go:158] Controller kube-system/heapster-v1.2.0 either never recorded expectations, or the ttl expired.
I1223 12:09:39.821155    7550 controller_ref_manager.go:79] Ignoring pod kube-system/kube-dns-v20-xojuw, it's owned by [v1/ReplicationController, name: kube-dns-v20, uid: 348ef075-c8de-11e6-b98e-08002750cdfb]
I1223 12:09:39.821159    7550 controller_ref_manager.go:79] Ignoring pod kube-system/monitoring-influxdb-grafana-v3-n3odm, it's owned by [v1/ReplicationController, name: monitoring-influxdb-grafana-v3, uid: d85c1a29-c8e5-11e6-85dd-08002750cdfb]
I1223 12:09:39.821162    7550 controller_ref_manager.go:79] Ignoring pod kube-system/php-apache-tx43w, it's owned by [v1/ReplicationController, name: php-apache, uid: eb9f0081-c8e5-11e6-85dd-08002750cdfb]
I1223 12:09:39.821169    7550 replication_controller.go:647] Finished syncing controller "kube-system/heapster-v1.2.0" (24.229µs)
I1223 12:09:39.821177    7550 replication_controller.go:653] Waiting for pods controller to sync, requeuing rc kube-system/kube-dns-v20
I1223 12:09:39.821181    7550 replication_controller.go:647] Finished syncing controller "kube-system/kube-dns-v20" (105.775808ms)
I1223 12:09:39.821189    7550 controller_utils.go:158] Controller kube-system/kube-dns-v20 either never recorded expectations, or the ttl expired.
I1223 12:09:39.821197    7550 controller_ref_manager.go:79] Ignoring pod kube-system/heapster-v1.2.0-uobac, it's owned by [v1/ReplicationController, name: heapster-v1.2.0, uid: d84f1d82-c8e5-11e6-85dd-08002750cdfb]
I1223 12:09:39.821202    7550 controller_ref_manager.go:79] Ignoring pod kube-system/monitoring-influxdb-grafana-v3-n3odm, it's owned by [v1/ReplicationController, name: monitoring-influxdb-grafana-v3, uid: d85c1a29-c8e5-11e6-85dd-08002750cdfb]
I1223 12:09:39.821210    7550 controller_ref_manager.go:79] Ignoring pod kube-system/php-apache-tx43w, it's owned by [v1/ReplicationController, name: php-apache, uid: eb9f0081-c8e5-11e6-85dd-08002750cdfb]
I1223 12:09:39.821216    7550 replication_controller.go:647] Finished syncing controller "kube-system/kube-dns-v20" (28.171µs)
I1223 12:09:39.821225    7550 replication_controller.go:653] Waiting for pods controller to sync, requeuing rc kube-system/monitoring-influxdb-grafana-v3
I1223 12:09:39.821228    7550 replication_controller.go:647] Finished syncing controller "kube-system/monitoring-influxdb-grafana-v3" (105.813634ms)
I1223 12:09:39.821237    7550 controller_utils.go:158] Controller kube-system/monitoring-influxdb-grafana-v3 either never recorded expectations, or the ttl expired.
I1223 12:09:39.821245    7550 controller_ref_manager.go:79] Ignoring pod kube-system/kube-dns-v20-xojuw, it's owned by [v1/ReplicationController, name: kube-dns-v20, uid: 348ef075-c8de-11e6-b98e-08002750cdfb]
I1223 12:09:39.821250    7550 controller_ref_manager.go:79] Ignoring pod kube-system/php-apache-tx43w, it's owned by [v1/ReplicationController, name: php-apache, uid: eb9f0081-c8e5-11e6-85dd-08002750cdfb]
I1223 12:09:39.821254    7550 controller_ref_manager.go:79] Ignoring pod kube-system/heapster-v1.2.0-uobac, it's owned by [v1/ReplicationController, name: heapster-v1.2.0, uid: d84f1d82-c8e5-11e6-85dd-08002750cdfb]
I1223 12:09:39.821260    7550 replication_controller.go:647] Finished syncing controller "kube-system/monitoring-influxdb-grafana-v3" (24.3µs)
I1223 12:09:39.824896    7550 endpoints_controller.go:334] Finished syncing service "default/sample" endpoints. (5.019009ms)
I1223 12:09:39.827740    7550 shared_informer.go:105] caches populated
I1223 12:09:39.827837    7550 controller_utils.go:158] Controller default/sample-deployment-3552496463 either never recorded expectations, or the ttl expired.
I1223 12:09:39.827882    7550 replica_set.go:565] Finished syncing replica set "default/sample-deployment-3552496463" (90.856µs)
I1223 12:09:39.827907    7550 shared_informer.go:105] caches populated
I1223 12:09:39.827923    7550 shared_informer.go:105] caches populated
I1223 12:09:39.827947    7550 shared_informer.go:105] caches populated
I1223 12:09:39.828439    7550 deployment_controller.go:313] Finished syncing deployment "default/sample-deployment" (469.096µs)
I1223 12:09:39.836621    7550 shared_informer.go:105] caches populated
I1223 12:09:39.836679    7550 serviceaccounts_controller.go:191] Finished syncing namespace "default" (6.669µs)
I1223 12:09:39.836694    7550 serviceaccounts_controller.go:191] Finished syncing namespace "kube-system" (1.403µs)
I1223 12:09:40.182807    7550 attach_detach_controller.go:539] processVolumesInUse for node "10.15.137.242"
I1223 12:09:41.692957    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:09:43.268133    7550 attach_detach_controller.go:539] processVolumesInUse for node "10.15.137.241"
I1223 12:09:43.697829    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:09:44.739637    7550 namespace_controller.go:206] Finished syncing namespace "kube-system" (435ns)
I1223 12:09:44.739681    7550 namespace_controller.go:206] Finished syncing namespace "default" (100ns)
I1223 12:09:44.822917    7550 nodecontroller.go:713] Node 10.15.137.241 ReadyCondition updated. Updating timestamp.
I1223 12:09:44.822997    7550 nodecontroller.go:713] Node 10.15.137.242 ReadyCondition updated. Updating timestamp.
I1223 12:09:45.702519    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:09:47.707105    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:09:49.711818    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:09:49.748292    7550 garbagecollector.go:772] Garbage Collector: All monitored resources synced. Proceeding to collect garbage
I1223 12:09:50.247914    7550 attach_detach_controller.go:539] processVolumesInUse for node "10.15.137.242"
I1223 12:09:51.716522    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:09:53.332455    7550 attach_detach_controller.go:539] processVolumesInUse for node "10.15.137.241"
I1223 12:09:53.720703    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:09:54.736418    7550 reflector.go:273] pkg/controller/volume/persistentvolume/pv_controller_base.go:159: forcing resync
I1223 12:09:54.736501    7550 reflector.go:273] pkg/controller/volume/persistentvolume/pv_controller_base.go:454: forcing resync
I1223 12:09:54.736521    7550 reflector.go:273] pkg/controller/volume/persistentvolume/pv_controller_base.go:455: forcing resync
I1223 12:09:54.825828    7550 nodecontroller.go:713] Node 10.15.137.241 ReadyCondition updated. Updating timestamp.
I1223 12:09:54.825919    7550 nodecontroller.go:713] Node 10.15.137.242 ReadyCondition updated. Updating timestamp.
I1223 12:09:55.117804    7550 replication_controller.go:378] Pod busybox updated, objectMeta {Name:busybox GenerateName: Namespace:kube-system SelfLink:/api/v1/namespaces/kube-system/pods/busybox UID:f427d5c2-c8e5-11e6-85dd-08002750cdfb ResourceVersion:26249 Generation:0 CreationTimestamp:2016-12-23 08:01:05 +0000 UTC DeletionTimestamp:<nil> DeletionGracePeriodSeconds:<nil> Labels:map[] Annotations:map[] OwnerReferences:[] Finalizers:[] ClusterName:} -> {Name:busybox GenerateName: Namespace:kube-system SelfLink:/api/v1/namespaces/kube-system/pods/busybox UID:f427d5c2-c8e5-11e6-85dd-08002750cdfb ResourceVersion:26299 Generation:0 CreationTimestamp:2016-12-23 08:01:05 +0000 UTC DeletionTimestamp:<nil> DeletionGracePeriodSeconds:<nil> Labels:map[] Annotations:map[] OwnerReferences:[] Finalizers:[] ClusterName:}.
I1223 12:09:55.117969    7550 replication_controller.go:255] No controllers found for pod busybox, replication manager will avoid syncing
I1223 12:09:55.117989    7550 daemoncontroller.go:328] Pod busybox updated.
I1223 12:09:55.118027    7550 daemoncontroller.go:257] No daemon sets found for pod busybox, daemon set controller will avoid syncing
I1223 12:09:55.118040    7550 jobcontroller.go:141] No jobs found for pod busybox, job controller will avoid syncing
I1223 12:09:55.118049    7550 replica_set.go:320] Pod busybox updated, objectMeta {Name:busybox GenerateName: Namespace:kube-system SelfLink:/api/v1/namespaces/kube-system/pods/busybox UID:f427d5c2-c8e5-11e6-85dd-08002750cdfb ResourceVersion:26249 Generation:0 CreationTimestamp:2016-12-23 08:01:05 +0000 UTC DeletionTimestamp:<nil> DeletionGracePeriodSeconds:<nil> Labels:map[] Annotations:map[] OwnerReferences:[] Finalizers:[] ClusterName:} -> {Name:busybox GenerateName: Namespace:kube-system SelfLink:/api/v1/namespaces/kube-system/pods/busybox UID:f427d5c2-c8e5-11e6-85dd-08002750cdfb ResourceVersion:26299 Generation:0 CreationTimestamp:2016-12-23 08:01:05 +0000 UTC DeletionTimestamp:<nil> DeletionGracePeriodSeconds:<nil> Labels:map[] Annotations:map[] OwnerReferences:[] Finalizers:[] ClusterName:}.
I1223 12:09:55.118106    7550 replica_set.go:196] No ReplicaSets found for pod busybox, ReplicaSet controller will avoid syncing
I1223 12:09:55.118115    7550 disruption.go:326] updatePod called on pod "busybox"
I1223 12:09:55.118122    7550 disruption.go:389] No PodDisruptionBudgets found for pod busybox, PodDisruptionBudget controller will avoid syncing.
I1223 12:09:55.118125    7550 disruption.go:329] No matching pdb for pod "busybox"
I1223 12:09:55.118134    7550 pet_set.go:239] No StatefulSets found for pod busybox, StatefulSet controller will avoid syncing
I1223 12:09:55.725405    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:09:57.731140    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:09:59.695410    7550 gc_controller.go:175] GC'ing orphaned
I1223 12:09:59.695441    7550 gc_controller.go:195] GC'ing unscheduled pods which are terminating.
I1223 12:09:59.736398    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:10:00.312195    7550 attach_detach_controller.go:539] processVolumesInUse for node "10.15.137.242"
I1223 12:10:01.741185    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:10:03.398002    7550 attach_detach_controller.go:539] processVolumesInUse for node "10.15.137.241"
I1223 12:10:03.746015    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:10:04.829690    7550 nodecontroller.go:713] Node 10.15.137.241 ReadyCondition updated. Updating timestamp.
I1223 12:10:04.829747    7550 nodecontroller.go:713] Node 10.15.137.242 ReadyCondition updated. Updating timestamp.
I1223 12:10:05.750250    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:10:07.754798    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:10:09.565711    7550 metrics_client.go:94] Heapster metrics result: {
  "metadata": {},
  "items": [
   {
    "metadata": {
     "name": "php-apache-tx43w",
     "namespace": "kube-system",
     "creationTimestamp": "2016-12-23T12:10:09Z"
    },
    "timestamp": "2016-12-23T12:10:00Z",
    "window": "1m0s",
    "containers": [
     {
      "name": "php-apache",
      "usage": {
       "cpu": "0",
       "memory": "9268Ki"
      }
     }
    ]
   }
  ]
 }
I1223 12:10:09.574451    7550 metrics_client.go:170] czq Heapster metrics path: /api/v1/model/namespaces/kube-system/pod-list/php-apache-tx43w/metrics/custom/qps
I1223 12:10:09.574471    7550 metrics_client.go:171] czq Heapster metrics result: {"items":[{"metrics":[{"timestamp":"2016-12-23T12:08:00Z","value":47},{"timestamp":"2016-12-23T12:09:00Z","value":28},{"timestamp":"2016-12-23T12:10:00Z","value":53}],"latestTimestamp":"2016-12-23T12:10:00Z"}]}
I1223 12:10:09.574477    7550 metrics_client.go:211] czq metrics_client.go collapseTimeSamples1 :2016-12-23 12:08:00 +0000 UTC, %!s(uint64=47)
I1223 12:10:09.574488    7550 metrics_client.go:211] czq metrics_client.go collapseTimeSamples1 :2016-12-23 12:09:00 +0000 UTC, %!s(uint64=28)
I1223 12:10:09.574492    7550 metrics_client.go:211] czq metrics_client.go collapseTimeSamples1 :2016-12-23 12:10:00 +0000 UTC, %!s(uint64=53)
I1223 12:10:09.574497    7550 metrics_client.go:217] czq metrics_client.go collapseTimeSamples2 :2016-12-23 12:10:00 +0000 UTC, 53
I1223 12:10:09.574503    7550 metrics_client.go:186] czq metrics_client.go GetRawMetric :php-apache-tx43w, 2016-12-23 12:10:00 +0000 UTC
I1223 12:10:09.574508    7550 metrics_client.go:188] czq metrics_client.go GetRawMetric <nil>
I1223 12:10:09.583021    7550 horizontal.go:379] Successfull rescale of php-apache, old size: 1, new size: 2, reason: Custom metric qps above target
I1223 12:10:09.585745    7550 replication_controller.go:322] Observed updated replication controller php-apache. Desired pod count change: 1->2
I1223 12:10:09.585777    7550 controller_utils.go:158] Controller kube-system/php-apache either never recorded expectations, or the ttl expired.
I1223 12:10:09.585796    7550 controller_ref_manager.go:79] Ignoring pod kube-system/heapster-v1.2.0-uobac, it's owned by [v1/ReplicationController, name: heapster-v1.2.0, uid: d84f1d82-c8e5-11e6-85dd-08002750cdfb]
I1223 12:10:09.585802    7550 controller_ref_manager.go:79] Ignoring pod kube-system/kube-dns-v20-xojuw, it's owned by [v1/ReplicationController, name: kube-dns-v20, uid: 348ef075-c8de-11e6-b98e-08002750cdfb]
I1223 12:10:09.585807    7550 controller_ref_manager.go:79] Ignoring pod kube-system/monitoring-influxdb-grafana-v3-n3odm, it's owned by [v1/ReplicationController, name: monitoring-influxdb-grafana-v3, uid: d85c1a29-c8e5-11e6-85dd-08002750cdfb]
I1223 12:10:09.585813    7550 controller_utils.go:175] Setting expectations &controller.ControlleeExpectations{add:1, del:0, key:"kube-system/php-apache", timestamp:time.Time{sec:63618091809, nsec:585811531, loc:(*time.Location)(0x5cacc20)}}
I1223 12:10:09.585840    7550 replication_controller.go:541] Too few "kube-system"/"php-apache" replicas, need 2, creating 1
I1223 12:10:09.593684    7550 controller_utils.go:512] Controller php-apache created pod php-apache-txnlk
I1223 12:10:09.593732    7550 replication_controller_utils.go:58] Updating replica count for rc: kube-system/php-apache, replicas 1->1 (need 2), fullyLabeledReplicas 1->1, readyReplicas 1->1, availableReplicas 1->1, sequence No: 7->8
I1223 12:10:09.594092    7550 event.go:217] Event(api.ObjectReference{Kind:"ReplicationController", Namespace:"kube-system", Name:"php-apache", UID:"eb9f0081-c8e5-11e6-85dd-08002750cdfb", APIVersion:"v1", ResourceVersion:"26319", FieldPath:""}): type: 'Normal' reason: 'SuccessfulCreate' Created pod: php-apache-txnlk
I1223 12:10:09.594236    7550 horizontal.go:438] Successfully updated status for php-apache
I1223 12:10:09.594744    7550 controller_utils.go:192] Lowered expectations &controller.ControlleeExpectations{add:0, del:0, key:"kube-system/php-apache", timestamp:time.Time{sec:63618091809, nsec:585811531, loc:(*time.Location)(0x5cacc20)}}
I1223 12:10:09.594775    7550 daemoncontroller.go:305] Pod php-apache-txnlk added.
I1223 12:10:09.594789    7550 daemoncontroller.go:257] No daemon sets found for pod php-apache-txnlk, daemon set controller will avoid syncing
I1223 12:10:09.594820    7550 jobcontroller.go:141] No jobs found for pod php-apache-txnlk, job controller will avoid syncing
I1223 12:10:09.594828    7550 replica_set.go:288] Pod php-apache-txnlk created: &api.Pod{TypeMeta:unversioned.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:api.ObjectMeta{Name:"php-apache-txnlk", GenerateName:"php-apache-", Namespace:"kube-system", SelfLink:"/api/v1/namespaces/kube-system/pods/php-apache-txnlk", UID:"bfc89e61-c908-11e6-8f89-08002750cdfb", ResourceVersion:"26322", Generation:0, CreationTimestamp:unversioned.Time{Time:time.Time{sec:63618091809, nsec:0, loc:(*time.Location)(0x5cacc20)}}, DeletionTimestamp:(*unversioned.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"app":"php-apache"}, Annotations:map[string]string{"kubernetes.io/created-by":"{\"kind\":\"SerializedReference\",\"apiVersion\":\"v1\",\"reference\":{\"kind\":\"ReplicationController\",\"namespace\":\"kube-system\",\"name\":\"php-apache\",\"uid\":\"eb9f0081-c8e5-11e6-85dd-08002750cdfb\",\"apiVersion\":\"v1\",\"resourceVersion\":\"26319\"}}\n"}, OwnerReferences:[]api.OwnerReference{api.OwnerReference{APIVersion:"v1", Kind:"ReplicationController", Name:"php-apache", UID:"eb9f0081-c8e5-11e6-85dd-08002750cdfb", Controller:(*bool)(0xc42026c26c)}}, Finalizers:[]string(nil), ClusterName:""}, Spec:api.PodSpec{Volumes:[]api.Volume{api.Volume{Name:"default-token-k9ean", VolumeSource:api.VolumeSource{HostPath:(*api.HostPathVolumeSource)(nil), EmptyDir:(*api.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*api.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*api.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*api.GitRepoVolumeSource)(nil), Secret:(*api.SecretVolumeSource)(0xc420b6c060), NFS:(*api.NFSVolumeSource)(nil), ISCSI:(*api.ISCSIVolumeSource)(nil), Glusterfs:(*api.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*api.PersistentVolumeClaimVolumeSource)(nil), RBD:(*api.RBDVolumeSource)(nil), Quobyte:(*api.QuobyteVolumeSource)(nil), FlexVolume:(*api.FlexVolumeSource)(nil), Cinder:(*api.CinderVolumeSource)(nil), CephFS:(*api.CephFSVolumeSource)(nil), Flocker:(*api.FlockerVolumeSource)(nil), DownwardAPI:(*api.DownwardAPIVolumeSource)(nil), FC:(*api.FCVolumeSource)(nil), AzureFile:(*api.AzureFileVolumeSource)(nil), ConfigMap:(*api.ConfigMapVolumeSource)(nil), VsphereVolume:(*api.VsphereVirtualDiskVolumeSource)(nil), AzureDisk:(*api.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*api.PhotonPersistentDiskVolumeSource)(nil)}}}, InitContainers:[]api.Container(nil), Containers:[]api.Container{api.Container{Name:"php-apache", Image:"10.213.42.254:10500/caozhiqiang1/hpa-example:2.5", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]api.ContainerPort{api.ContainerPort{Name:"", HostPort:8500, ContainerPort:80, Protocol:"TCP", HostIP:""}}, Env:[]api.EnvVar(nil), Resources:api.ResourceRequirements{Limits:api.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:209715200, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"", Format:"BinarySI"}}, Requests:api.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:209715200, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"", Format:"BinarySI"}}}, VolumeMounts:[]api.VolumeMount{api.VolumeMount{Name:"default-token-k9ean", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:""}}, LivenessProbe:(*api.Probe)(nil), ReadinessProbe:(*api.Probe)(nil), Lifecycle:(*api.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", ImagePullPolicy:"IfNotPresent", SecurityContext:(*api.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc42026c2f8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", NodeName:"", SecurityContext:(*api.PodSecurityContext)(0xc4210e5700), ImagePullSecrets:[]api.LocalObjectReference(nil), Hostname:"", Subdomain:""}, Status:api.PodStatus{Phase:"Pending", Conditions:[]api.PodCondition(nil), Message:"", Reason:"", HostIP:"", PodIP:"", StartTime:(*unversioned.Time)(nil), InitContainerStatuses:[]api.ContainerStatus(nil), ContainerStatuses:[]api.ContainerStatus(nil)}}.
I1223 12:10:09.594936    7550 replica_set.go:196] No ReplicaSets found for pod php-apache-txnlk, ReplicaSet controller will avoid syncing
I1223 12:10:09.594945    7550 disruption.go:314] addPod called on pod "php-apache-txnlk"
I1223 12:10:09.594966    7550 disruption.go:389] No PodDisruptionBudgets found for pod php-apache-txnlk, PodDisruptionBudget controller will avoid syncing.
I1223 12:10:09.594969    7550 disruption.go:317] No matching pdb for pod "php-apache-txnlk"
I1223 12:10:09.594977    7550 pet_set.go:160] Pod php-apache-txnlk created, labels: map[app:php-apache]
I1223 12:10:09.594994    7550 pet_set.go:239] No StatefulSets found for pod php-apache-txnlk, StatefulSet controller will avoid syncing
I1223 12:10:09.601532    7550 endpoints_controller.go:334] Finished syncing service "kube-system/php-apache" endpoints. (6.891336ms)
I1223 12:10:09.606364    7550 replication_controller.go:647] Finished syncing controller "kube-system/php-apache" (20.590433ms)
I1223 12:10:09.606407    7550 controller_ref_manager.go:79] Ignoring pod kube-system/heapster-v1.2.0-uobac, it's owned by [v1/ReplicationController, name: heapster-v1.2.0, uid: d84f1d82-c8e5-11e6-85dd-08002750cdfb]
I1223 12:10:09.606415    7550 controller_ref_manager.go:79] Ignoring pod kube-system/kube-dns-v20-xojuw, it's owned by [v1/ReplicationController, name: kube-dns-v20, uid: 348ef075-c8de-11e6-b98e-08002750cdfb]
I1223 12:10:09.606419    7550 controller_ref_manager.go:79] Ignoring pod kube-system/monitoring-influxdb-grafana-v3-n3odm, it's owned by [v1/ReplicationController, name: monitoring-influxdb-grafana-v3, uid: d85c1a29-c8e5-11e6-85dd-08002750cdfb]
I1223 12:10:09.606435    7550 replication_controller_utils.go:58] Updating replica count for rc: kube-system/php-apache, replicas 1->2 (need 2), fullyLabeledReplicas 1->2, readyReplicas 1->1, availableReplicas 1->1, sequence No: 7->8
I1223 12:10:09.609991    7550 replication_controller.go:322] Observed updated replication controller php-apache. Desired pod count change: 2->2
I1223 12:10:09.610098    7550 metrics_client.go:94] Heapster metrics result: {
  "metadata": {},
  "items": [
   {
    "metadata": {
     "name": "php-apache-tx43w",
     "namespace": "kube-system",
     "creationTimestamp": "2016-12-23T12:10:09Z"
    },
    "timestamp": "2016-12-23T12:10:00Z",
    "window": "1m0s",
    "containers": [
     {
      "name": "php-apache",
      "usage": {
       "cpu": "0",
       "memory": "9268Ki"
      }
     }
    ]
   }
  ]
 }
I1223 12:10:09.612782    7550 replication_controller.go:378] Pod php-apache-txnlk updated, objectMeta {Name:php-apache-txnlk GenerateName:php-apache- Namespace:kube-system SelfLink:/api/v1/namespaces/kube-system/pods/php-apache-txnlk UID:bfc89e61-c908-11e6-8f89-08002750cdfb ResourceVersion:26322 Generation:0 CreationTimestamp:2016-12-23 12:10:09 +0000 UTC DeletionTimestamp:<nil> DeletionGracePeriodSeconds:<nil> Labels:map[app:php-apache] Annotations:map[kubernetes.io/created-by:{"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicationController","namespace":"kube-system","name":"php-apache","uid":"eb9f0081-c8e5-11e6-85dd-08002750cdfb","apiVersion":"v1","resourceVersion":"26319"}}
] OwnerReferences:[{APIVersion:v1 Kind:ReplicationController Name:php-apache UID:eb9f0081-c8e5-11e6-85dd-08002750cdfb Controller:0xc42026c26c}] Finalizers:[] ClusterName:} -> {Name:php-apache-txnlk GenerateName:php-apache- Namespace:kube-system SelfLink:/api/v1/namespaces/kube-system/pods/php-apache-txnlk UID:bfc89e61-c908-11e6-8f89-08002750cdfb ResourceVersion:26324 Generation:0 CreationTimestamp:2016-12-23 12:10:09 +0000 UTC DeletionTimestamp:<nil> DeletionGracePeriodSeconds:<nil> Labels:map[app:php-apache] Annotations:map[kubernetes.io/created-by:{"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicationController","namespace":"kube-system","name":"php-apache","uid":"eb9f0081-c8e5-11e6-85dd-08002750cdfb","apiVersion":"v1","resourceVersion":"26319"}}
] OwnerReferences:[{APIVersion:v1 Kind:ReplicationController Name:php-apache UID:eb9f0081-c8e5-11e6-85dd-08002750cdfb Controller:0xc42022cbfc}] Finalizers:[] ClusterName:}.
I1223 12:10:09.612859    7550 daemoncontroller.go:328] Pod php-apache-txnlk updated.
I1223 12:10:09.612874    7550 daemoncontroller.go:257] No daemon sets found for pod php-apache-txnlk, daemon set controller will avoid syncing
I1223 12:10:09.612887    7550 jobcontroller.go:141] No jobs found for pod php-apache-txnlk, job controller will avoid syncing
I1223 12:10:09.612895    7550 replica_set.go:320] Pod php-apache-txnlk updated, objectMeta {Name:php-apache-txnlk GenerateName:php-apache- Namespace:kube-system SelfLink:/api/v1/namespaces/kube-system/pods/php-apache-txnlk UID:bfc89e61-c908-11e6-8f89-08002750cdfb ResourceVersion:26322 Generation:0 CreationTimestamp:2016-12-23 12:10:09 +0000 UTC DeletionTimestamp:<nil> DeletionGracePeriodSeconds:<nil> Labels:map[app:php-apache] Annotations:map[kubernetes.io/created-by:{"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicationController","namespace":"kube-system","name":"php-apache","uid":"eb9f0081-c8e5-11e6-85dd-08002750cdfb","apiVersion":"v1","resourceVersion":"26319"}}
] OwnerReferences:[{APIVersion:v1 Kind:ReplicationController Name:php-apache UID:eb9f0081-c8e5-11e6-85dd-08002750cdfb Controller:0xc42026c26c}] Finalizers:[] ClusterName:} -> {Name:php-apache-txnlk GenerateName:php-apache- Namespace:kube-system SelfLink:/api/v1/namespaces/kube-system/pods/php-apache-txnlk UID:bfc89e61-c908-11e6-8f89-08002750cdfb ResourceVersion:26324 Generation:0 CreationTimestamp:2016-12-23 12:10:09 +0000 UTC DeletionTimestamp:<nil> DeletionGracePeriodSeconds:<nil> Labels:map[app:php-apache] Annotations:map[kubernetes.io/created-by:{"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicationController","namespace":"kube-system","name":"php-apache","uid":"eb9f0081-c8e5-11e6-85dd-08002750cdfb","apiVersion":"v1","resourceVersion":"26319"}}
] OwnerReferences:[{APIVersion:v1 Kind:ReplicationController Name:php-apache UID:eb9f0081-c8e5-11e6-85dd-08002750cdfb Controller:0xc42022cbfc}] Finalizers:[] ClusterName:}.
I1223 12:10:09.612935    7550 replica_set.go:196] No ReplicaSets found for pod php-apache-txnlk, ReplicaSet controller will avoid syncing
I1223 12:10:09.612943    7550 disruption.go:326] updatePod called on pod "php-apache-txnlk"
I1223 12:10:09.612950    7550 disruption.go:389] No PodDisruptionBudgets found for pod php-apache-txnlk, PodDisruptionBudget controller will avoid syncing.
I1223 12:10:09.612953    7550 disruption.go:329] No matching pdb for pod "php-apache-txnlk"
I1223 12:10:09.612961    7550 pet_set.go:239] No StatefulSets found for pod php-apache-txnlk, StatefulSet controller will avoid syncing
I1223 12:10:09.617663    7550 replication_controller_utils.go:58] Updating replica count for rc: kube-system/php-apache, replicas 1->2 (need 2), fullyLabeledReplicas 1->2, readyReplicas 1->1, availableReplicas 1->1, sequence No: 8->8
I1223 12:10:09.617832    7550 endpoints_controller.go:334] Finished syncing service "kube-system/php-apache" endpoints. (5.149651ms)
I1223 12:10:09.631175    7550 metrics_client.go:170] czq Heapster metrics path: /api/v1/model/namespaces/kube-system/pod-list/php-apache-tx43w,php-apache-txnlk/metrics/custom/qps
I1223 12:10:09.631191    7550 metrics_client.go:171] czq Heapster metrics result: {"items":[{"metrics":[{"timestamp":"2016-12-23T12:08:00Z","value":47},{"timestamp":"2016-12-23T12:09:00Z","value":28},{"timestamp":"2016-12-23T12:10:00Z","value":53}],"latestTimestamp":"2016-12-23T12:10:00Z"},{"metrics":[],"latestTimestamp":"0001-01-01T00:00:00Z"}]}
I1223 12:10:09.631196    7550 metrics_client.go:211] czq metrics_client.go collapseTimeSamples1 :2016-12-23 12:08:00 +0000 UTC, %!s(uint64=47)
I1223 12:10:09.631206    7550 metrics_client.go:211] czq metrics_client.go collapseTimeSamples1 :2016-12-23 12:09:00 +0000 UTC, %!s(uint64=28)
I1223 12:10:09.631210    7550 metrics_client.go:211] czq metrics_client.go collapseTimeSamples1 :2016-12-23 12:10:00 +0000 UTC, %!s(uint64=53)
I1223 12:10:09.631215    7550 metrics_client.go:217] czq metrics_client.go collapseTimeSamples2 :2016-12-23 12:10:00 +0000 UTC, 53
I1223 12:10:09.631220    7550 metrics_client.go:186] czq metrics_client.go GetRawMetric :php-apache-tx43w, 2016-12-23 12:10:00 +0000 UTC
I1223 12:10:09.631225    7550 metrics_client.go:188] czq metrics_client.go GetRawMetric <nil>
I1223 12:10:09.633567    7550 replication_controller.go:378] Pod php-apache-txnlk updated, objectMeta {Name:php-apache-txnlk GenerateName:php-apache- Namespace:kube-system SelfLink:/api/v1/namespaces/kube-system/pods/php-apache-txnlk UID:bfc89e61-c908-11e6-8f89-08002750cdfb ResourceVersion:26324 Generation:0 CreationTimestamp:2016-12-23 12:10:09 +0000 UTC DeletionTimestamp:<nil> DeletionGracePeriodSeconds:<nil> Labels:map[app:php-apache] Annotations:map[kubernetes.io/created-by:{"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicationController","namespace":"kube-system","name":"php-apache","uid":"eb9f0081-c8e5-11e6-85dd-08002750cdfb","apiVersion":"v1","resourceVersion":"26319"}}
] OwnerReferences:[{APIVersion:v1 Kind:ReplicationController Name:php-apache UID:eb9f0081-c8e5-11e6-85dd-08002750cdfb Controller:0xc42022cbfc}] Finalizers:[] ClusterName:} -> {Name:php-apache-txnlk GenerateName:php-apache- Namespace:kube-system SelfLink:/api/v1/namespaces/kube-system/pods/php-apache-txnlk UID:bfc89e61-c908-11e6-8f89-08002750cdfb ResourceVersion:26329 Generation:0 CreationTimestamp:2016-12-23 12:10:09 +0000 UTC DeletionTimestamp:<nil> DeletionGracePeriodSeconds:<nil> Labels:map[app:php-apache] Annotations:map[kubernetes.io/created-by:{"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicationController","namespace":"kube-system","name":"php-apache","uid":"eb9f0081-c8e5-11e6-85dd-08002750cdfb","apiVersion":"v1","resourceVersion":"26319"}}
] OwnerReferences:[{APIVersion:v1 Kind:ReplicationController Name:php-apache UID:eb9f0081-c8e5-11e6-85dd-08002750cdfb Controller:0xc4201be6ac}] Finalizers:[] ClusterName:}.
I1223 12:10:09.633641    7550 daemoncontroller.go:328] Pod php-apache-txnlk updated.
I1223 12:10:09.633656    7550 daemoncontroller.go:257] No daemon sets found for pod php-apache-txnlk, daemon set controller will avoid syncing
I1223 12:10:09.633685    7550 jobcontroller.go:141] No jobs found for pod php-apache-txnlk, job controller will avoid syncing
I1223 12:10:09.633698    7550 replica_set.go:320] Pod php-apache-txnlk updated, objectMeta {Name:php-apache-txnlk GenerateName:php-apache- Namespace:kube-system SelfLink:/api/v1/namespaces/kube-system/pods/php-apache-txnlk UID:bfc89e61-c908-11e6-8f89-08002750cdfb ResourceVersion:26324 Generation:0 CreationTimestamp:2016-12-23 12:10:09 +0000 UTC DeletionTimestamp:<nil> DeletionGracePeriodSeconds:<nil> Labels:map[app:php-apache] Annotations:map[kubernetes.io/created-by:{"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicationController","namespace":"kube-system","name":"php-apache","uid":"eb9f0081-c8e5-11e6-85dd-08002750cdfb","apiVersion":"v1","resourceVersion":"26319"}}
] OwnerReferences:[{APIVersion:v1 Kind:ReplicationController Name:php-apache UID:eb9f0081-c8e5-11e6-85dd-08002750cdfb Controller:0xc42022cbfc}] Finalizers:[] ClusterName:} -> {Name:php-apache-txnlk GenerateName:php-apache- Namespace:kube-system SelfLink:/api/v1/namespaces/kube-system/pods/php-apache-txnlk UID:bfc89e61-c908-11e6-8f89-08002750cdfb ResourceVersion:26329 Generation:0 CreationTimestamp:2016-12-23 12:10:09 +0000 UTC DeletionTimestamp:<nil> DeletionGracePeriodSeconds:<nil> Labels:map[app:php-apache] Annotations:map[kubernetes.io/created-by:{"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicationController","namespace":"kube-system","name":"php-apache","uid":"eb9f0081-c8e5-11e6-85dd-08002750cdfb","apiVersion":"v1","resourceVersion":"26319"}}
] OwnerReferences:[{APIVersion:v1 Kind:ReplicationController Name:php-apache UID:eb9f0081-c8e5-11e6-85dd-08002750cdfb Controller:0xc4201be6ac}] Finalizers:[] ClusterName:}.
I1223 12:10:09.633740    7550 replica_set.go:196] No ReplicaSets found for pod php-apache-txnlk, ReplicaSet controller will avoid syncing
I1223 12:10:09.633750    7550 disruption.go:326] updatePod called on pod "php-apache-txnlk"
I1223 12:10:09.633757    7550 disruption.go:389] No PodDisruptionBudgets found for pod php-apache-txnlk, PodDisruptionBudget controller will avoid syncing.
I1223 12:10:09.633760    7550 disruption.go:329] No matching pdb for pod "php-apache-txnlk"
I1223 12:10:09.633769    7550 pet_set.go:239] No StatefulSets found for pod php-apache-txnlk, StatefulSet controller will avoid syncing
I1223 12:10:09.636342    7550 replication_controller.go:322] Observed updated replication controller php-apache. Desired pod count change: 2->2
I1223 12:10:09.636361    7550 replication_controller.go:338] Observed updated replica count for rc: php-apache, 1->2
I1223 12:10:09.636662    7550 replication_controller.go:647] Finished syncing controller "kube-system/php-apache" (30.270651ms)
I1223 12:10:09.636695    7550 controller_ref_manager.go:79] Ignoring pod kube-system/kube-dns-v20-xojuw, it's owned by [v1/ReplicationController, name: kube-dns-v20, uid: 348ef075-c8de-11e6-b98e-08002750cdfb]
I1223 12:10:09.636701    7550 controller_ref_manager.go:79] Ignoring pod kube-system/monitoring-influxdb-grafana-v3-n3odm, it's owned by [v1/ReplicationController, name: monitoring-influxdb-grafana-v3, uid: d85c1a29-c8e5-11e6-85dd-08002750cdfb]
I1223 12:10:09.636707    7550 controller_ref_manager.go:79] Ignoring pod kube-system/heapster-v1.2.0-uobac, it's owned by [v1/ReplicationController, name: heapster-v1.2.0, uid: d84f1d82-c8e5-11e6-85dd-08002750cdfb]
I1223 12:10:09.636716    7550 replication_controller.go:647] Finished syncing controller "kube-system/php-apache" (35.494µs)
I1223 12:10:09.638178    7550 endpoints_controller.go:334] Finished syncing service "kube-system/php-apache" endpoints. (4.741613ms)
I1223 12:10:09.643349    7550 horizontal.go:438] Successfully updated status for php-apache
I1223 12:10:09.646235    7550 metrics_client.go:94] Heapster metrics result: {
  "metadata": {},
  "items": [
   {
    "metadata": {
     "name": "php-apache-tx43w",
     "namespace": "kube-system",
     "creationTimestamp": "2016-12-23T12:10:09Z"
    },
    "timestamp": "2016-12-23T12:10:00Z",
    "window": "1m0s",
    "containers": [
     {
      "name": "php-apache",
      "usage": {
       "cpu": "0",
       "memory": "9268Ki"
      }
     }
    ]
   }
  ]
 }
I1223 12:10:09.652517    7550 metrics_client.go:170] czq Heapster metrics path: /api/v1/model/namespaces/kube-system/pod-list/php-apache-tx43w,php-apache-txnlk/metrics/custom/qps
I1223 12:10:09.652536    7550 metrics_client.go:171] czq Heapster metrics result: {"items":[{"metrics":[{"timestamp":"2016-12-23T12:08:00Z","value":47},{"timestamp":"2016-12-23T12:09:00Z","value":28},{"timestamp":"2016-12-23T12:10:00Z","value":53}],"latestTimestamp":"2016-12-23T12:10:00Z"},{"metrics":[],"latestTimestamp":"0001-01-01T00:00:00Z"}]}
I1223 12:10:09.652541    7550 metrics_client.go:211] czq metrics_client.go collapseTimeSamples1 :2016-12-23 12:08:00 +0000 UTC, %!s(uint64=47)
I1223 12:10:09.652550    7550 metrics_client.go:211] czq metrics_client.go collapseTimeSamples1 :2016-12-23 12:09:00 +0000 UTC, %!s(uint64=28)
I1223 12:10:09.652555    7550 metrics_client.go:211] czq metrics_client.go collapseTimeSamples1 :2016-12-23 12:10:00 +0000 UTC, %!s(uint64=53)
I1223 12:10:09.652560    7550 metrics_client.go:217] czq metrics_client.go collapseTimeSamples2 :2016-12-23 12:10:00 +0000 UTC, 53
I1223 12:10:09.652565    7550 metrics_client.go:186] czq metrics_client.go GetRawMetric :php-apache-tx43w, 2016-12-23 12:10:00 +0000 UTC
I1223 12:10:09.652570    7550 metrics_client.go:188] czq metrics_client.go GetRawMetric <nil>
I1223 12:10:09.658530    7550 horizontal.go:438] Successfully updated status for php-apache
I1223 12:10:09.661157    7550 metrics_client.go:94] Heapster metrics result: {
  "metadata": {},
  "items": [
   {
    "metadata": {
     "name": "php-apache-tx43w",
     "namespace": "kube-system",
     "creationTimestamp": "2016-12-23T12:10:09Z"
    },
    "timestamp": "2016-12-23T12:10:00Z",
    "window": "1m0s",
    "containers": [
     {
      "name": "php-apache",
      "usage": {
       "cpu": "0",
       "memory": "9268Ki"
      }
     }
    ]
   }
  ]
 }
I1223 12:10:09.696024    7550 reflector.go:273] pkg/controller/endpoint/endpoints_controller.go:160: forcing resync
I1223 12:10:09.696319    7550 endpoints_controller.go:334] Finished syncing service "default/kubernetes" endpoints. (1.707µs)
I1223 12:10:09.699951    7550 endpoints_controller.go:334] Finished syncing service "kube-system/php-apache" endpoints. (3.803328ms)
I1223 12:10:09.700148    7550 endpoints_controller.go:334] Finished syncing service "default/sample" endpoints. (3.811137ms)
I1223 12:10:09.700210    7550 endpoints_controller.go:334] Finished syncing service "kube-system/kube-dns" endpoints. (3.58771ms)
I1223 12:10:09.700330    7550 endpoints_controller.go:334] Finished syncing service "kube-system/heapster" endpoints. (3.913472ms)
I1223 12:10:09.700402    7550 endpoints_controller.go:334] Finished syncing service "kube-system/monitoring-grafana" endpoints. (3.623793ms)
I1223 12:10:09.701099    7550 endpoints_controller.go:334] Finished syncing service "kube-system/monitoring-influxdb" endpoints. (1.119083ms)
I1223 12:10:09.725380    7550 request.go:632] Throttling request took 62.633326ms, request: PATCH:http://10.15.137.240:8080/api/v1/namespaces/kube-system/events/php-apache.1492e15b4f56a490
I1223 12:10:09.727299    7550 reflector.go:273] pkg/controller/podautoscaler/horizontal.go:133: forcing resync
I1223 12:10:09.729274    7550 reflector.go:273] pkg/controller/disruption/disruption.go:281: forcing resync
I1223 12:10:09.729493    7550 reflector.go:273] pkg/controller/disruption/disruption.go:283: forcing resync
I1223 12:10:09.729862    7550 reflector.go:273] pkg/controller/disruption/disruption.go:284: forcing resync
I1223 12:10:09.730047    7550 reflector.go:273] pkg/controller/disruption/disruption.go:285: forcing resync
I1223 12:10:09.730476    7550 reflector.go:273] pkg/controller/petset/pet_set.go:148: forcing resync
I1223 12:10:09.736722    7550 reflector.go:273] pkg/controller/volume/persistentvolume/pv_controller_base.go:159: forcing resync
I1223 12:10:09.736775    7550 reflector.go:273] pkg/controller/volume/persistentvolume/pv_controller_base.go:454: forcing resync
I1223 12:10:09.736785    7550 reflector.go:273] pkg/controller/volume/persistentvolume/pv_controller_base.go:455: forcing resync
I1223 12:10:09.748122    7550 reflector.go:273] pkg/controller/informers/factory.go:89: forcing resync
I1223 12:10:09.748291    7550 deployment_controller.go:154] Updating deployment sample-deployment
I1223 12:10:09.748780    7550 deployment_controller.go:313] Finished syncing deployment "default/sample-deployment" (468.33µs)
I1223 12:10:09.759946    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:10:09.775797    7550 request.go:632] Throttling request took 98.464794ms, request: GET:http://10.15.137.240:8080/api/v1/proxy/namespaces/kube-system/services/http:heapster:/api/v1/model/namespaces/kube-system/pod-list/php-apache-tx43w,php-apache-txnlk/metrics/custom/qps?start=2016-12-23T12%3A05%3A09Z
I1223 12:10:09.778360    7550 metrics_client.go:170] czq Heapster metrics path: /api/v1/model/namespaces/kube-system/pod-list/php-apache-tx43w,php-apache-txnlk/metrics/custom/qps
I1223 12:10:09.778383    7550 metrics_client.go:171] czq Heapster metrics result: {"items":[{"metrics":[{"timestamp":"2016-12-23T12:08:00Z","value":47},{"timestamp":"2016-12-23T12:09:00Z","value":28},{"timestamp":"2016-12-23T12:10:00Z","value":53}],"latestTimestamp":"2016-12-23T12:10:00Z"},{"metrics":[],"latestTimestamp":"0001-01-01T00:00:00Z"}]}
I1223 12:10:09.778390    7550 metrics_client.go:211] czq metrics_client.go collapseTimeSamples1 :2016-12-23 12:08:00 +0000 UTC, %!s(uint64=47)
I1223 12:10:09.778400    7550 metrics_client.go:211] czq metrics_client.go collapseTimeSamples1 :2016-12-23 12:09:00 +0000 UTC, %!s(uint64=28)
I1223 12:10:09.778405    7550 metrics_client.go:211] czq metrics_client.go collapseTimeSamples1 :2016-12-23 12:10:00 +0000 UTC, %!s(uint64=53)
I1223 12:10:09.778410    7550 metrics_client.go:217] czq metrics_client.go collapseTimeSamples2 :2016-12-23 12:10:00 +0000 UTC, 53
I1223 12:10:09.778416    7550 metrics_client.go:186] czq metrics_client.go GetRawMetric :php-apache-tx43w, 2016-12-23 12:10:00 +0000 UTC
I1223 12:10:09.778421    7550 metrics_client.go:188] czq metrics_client.go GetRawMetric <nil>
I1223 12:10:09.877196    7550 horizontal.go:438] Successfully updated status for php-apache
I1223 12:10:09.926100    7550 request.go:632] Throttling request took 98.499256ms, request: PATCH:http://10.15.137.240:8080/api/v1/namespaces/kube-system/events/php-apache.1492e15b4fb6e9b2
I1223 12:10:09.976124    7550 request.go:632] Throttling request took 98.840717ms, request: GET:http://10.15.137.240:8080/apis/extensions/v1beta1/namespaces/kube-system/replicationcontrollers/php-apache/scale
I1223 12:10:10.028128    7550 metrics_client.go:94] Heapster metrics result: {
  "metadata": {},
  "items": [
   {
    "metadata": {
     "name": "php-apache-tx43w",
     "namespace": "kube-system",
     "creationTimestamp": "2016-12-23T12:10:10Z"
    },
    "timestamp": "2016-12-23T12:10:00Z",
    "window": "1m0s",
    "containers": [
     {
      "name": "php-apache",
      "usage": {
       "cpu": "0",
       "memory": "9268Ki"
      }
     }
    ]
   }
  ]
 }
I1223 12:10:10.175794    7550 request.go:632] Throttling request took 97.778035ms, request: PATCH:http://10.15.137.240:8080/api/v1/namespaces/kube-system/events/php-apache.1492e15b4f56a490
I1223 12:10:10.225822    7550 request.go:632] Throttling request took 98.006528ms, request: GET:http://10.15.137.240:8080/api/v1/proxy/namespaces/kube-system/services/http:heapster:/api/v1/model/namespaces/kube-system/pod-list/php-apache-tx43w,php-apache-txnlk/metrics/custom/qps?start=2016-12-23T12%3A05%3A10Z
I1223 12:10:10.228400    7550 metrics_client.go:170] czq Heapster metrics path: /api/v1/model/namespaces/kube-system/pod-list/php-apache-tx43w,php-apache-txnlk/metrics/custom/qps
I1223 12:10:10.228427    7550 metrics_client.go:171] czq Heapster metrics result: {"items":[{"metrics":[{"timestamp":"2016-12-23T12:08:00Z","value":47},{"timestamp":"2016-12-23T12:09:00Z","value":28},{"timestamp":"2016-12-23T12:10:00Z","value":53}],"latestTimestamp":"2016-12-23T12:10:00Z"},{"metrics":[],"latestTimestamp":"0001-01-01T00:00:00Z"}]}
I1223 12:10:10.228442    7550 metrics_client.go:211] czq metrics_client.go collapseTimeSamples1 :2016-12-23 12:08:00 +0000 UTC, %!s(uint64=47)
I1223 12:10:10.228455    7550 metrics_client.go:211] czq metrics_client.go collapseTimeSamples1 :2016-12-23 12:09:00 +0000 UTC, %!s(uint64=28)
I1223 12:10:10.228460    7550 metrics_client.go:211] czq metrics_client.go collapseTimeSamples1 :2016-12-23 12:10:00 +0000 UTC, %!s(uint64=53)
I1223 12:10:10.228464    7550 metrics_client.go:217] czq metrics_client.go collapseTimeSamples2 :2016-12-23 12:10:00 +0000 UTC, 53
I1223 12:10:10.228470    7550 metrics_client.go:186] czq metrics_client.go GetRawMetric :php-apache-tx43w, 2016-12-23 12:10:00 +0000 UTC
I1223 12:10:10.228475    7550 metrics_client.go:188] czq metrics_client.go GetRawMetric <nil>
I1223 12:10:10.327662    7550 horizontal.go:438] Successfully updated status for php-apache
I1223 12:10:10.377310    7550 request.go:632] Throttling request took 99.573821ms, request: PATCH:http://10.15.137.240:8080/api/v1/namespaces/kube-system/events/php-apache.1492e15b4fb6e9b2
I1223 12:10:10.377930    7550 attach_detach_controller.go:539] processVolumesInUse for node "10.15.137.242"
I1223 12:10:11.519083    7550 replication_controller.go:378] Pod php-apache-txnlk updated, objectMeta {Name:php-apache-txnlk GenerateName:php-apache- Namespace:kube-system SelfLink:/api/v1/namespaces/kube-system/pods/php-apache-txnlk UID:bfc89e61-c908-11e6-8f89-08002750cdfb ResourceVersion:26329 Generation:0 CreationTimestamp:2016-12-23 12:10:09 +0000 UTC DeletionTimestamp:<nil> DeletionGracePeriodSeconds:<nil> Labels:map[app:php-apache] Annotations:map[kubernetes.io/created-by:{"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicationController","namespace":"kube-system","name":"php-apache","uid":"eb9f0081-c8e5-11e6-85dd-08002750cdfb","apiVersion":"v1","resourceVersion":"26319"}}
] OwnerReferences:[{APIVersion:v1 Kind:ReplicationController Name:php-apache UID:eb9f0081-c8e5-11e6-85dd-08002750cdfb Controller:0xc4201be6ac}] Finalizers:[] ClusterName:} -> {Name:php-apache-txnlk GenerateName:php-apache- Namespace:kube-system SelfLink:/api/v1/namespaces/kube-system/pods/php-apache-txnlk UID:bfc89e61-c908-11e6-8f89-08002750cdfb ResourceVersion:26345 Generation:0 CreationTimestamp:2016-12-23 12:10:09 +0000 UTC DeletionTimestamp:<nil> DeletionGracePeriodSeconds:<nil> Labels:map[app:php-apache] Annotations:map[kubernetes.io/created-by:{"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicationController","namespace":"kube-system","name":"php-apache","uid":"eb9f0081-c8e5-11e6-85dd-08002750cdfb","apiVersion":"v1","resourceVersion":"26319"}}
] OwnerReferences:[{APIVersion:v1 Kind:ReplicationController Name:php-apache UID:eb9f0081-c8e5-11e6-85dd-08002750cdfb Controller:0xc421b14e4c}] Finalizers:[] ClusterName:}.
I1223 12:10:11.519180    7550 controller_ref_manager.go:79] Ignoring pod kube-system/heapster-v1.2.0-uobac, it's owned by [v1/ReplicationController, name: heapster-v1.2.0, uid: d84f1d82-c8e5-11e6-85dd-08002750cdfb]
I1223 12:10:11.519190    7550 controller_ref_manager.go:79] Ignoring pod kube-system/kube-dns-v20-xojuw, it's owned by [v1/ReplicationController, name: kube-dns-v20, uid: 348ef075-c8de-11e6-b98e-08002750cdfb]
I1223 12:10:11.519195    7550 controller_ref_manager.go:79] Ignoring pod kube-system/monitoring-influxdb-grafana-v3-n3odm, it's owned by [v1/ReplicationController, name: monitoring-influxdb-grafana-v3, uid: d85c1a29-c8e5-11e6-85dd-08002750cdfb]
I1223 12:10:11.519208    7550 replication_controller_utils.go:58] Updating replica count for rc: kube-system/php-apache, replicas 2->2 (need 2), fullyLabeledReplicas 2->2, readyReplicas 1->2, availableReplicas 1->2, sequence No: 8->8
I1223 12:10:11.519355    7550 daemoncontroller.go:328] Pod php-apache-txnlk updated.
I1223 12:10:11.519382    7550 daemoncontroller.go:257] No daemon sets found for pod php-apache-txnlk, daemon set controller will avoid syncing
I1223 12:10:11.519397    7550 jobcontroller.go:141] No jobs found for pod php-apache-txnlk, job controller will avoid syncing
I1223 12:10:11.519412    7550 replica_set.go:320] Pod php-apache-txnlk updated, objectMeta {Name:php-apache-txnlk GenerateName:php-apache- Namespace:kube-system SelfLink:/api/v1/namespaces/kube-system/pods/php-apache-txnlk UID:bfc89e61-c908-11e6-8f89-08002750cdfb ResourceVersion:26329 Generation:0 CreationTimestamp:2016-12-23 12:10:09 +0000 UTC DeletionTimestamp:<nil> DeletionGracePeriodSeconds:<nil> Labels:map[app:php-apache] Annotations:map[kubernetes.io/created-by:{"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicationController","namespace":"kube-system","name":"php-apache","uid":"eb9f0081-c8e5-11e6-85dd-08002750cdfb","apiVersion":"v1","resourceVersion":"26319"}}
] OwnerReferences:[{APIVersion:v1 Kind:ReplicationController Name:php-apache UID:eb9f0081-c8e5-11e6-85dd-08002750cdfb Controller:0xc4201be6ac}] Finalizers:[] ClusterName:} -> {Name:php-apache-txnlk GenerateName:php-apache- Namespace:kube-system SelfLink:/api/v1/namespaces/kube-system/pods/php-apache-txnlk UID:bfc89e61-c908-11e6-8f89-08002750cdfb ResourceVersion:26345 Generation:0 CreationTimestamp:2016-12-23 12:10:09 +0000 UTC DeletionTimestamp:<nil> DeletionGracePeriodSeconds:<nil> Labels:map[app:php-apache] Annotations:map[kubernetes.io/created-by:{"kind":"SerializedReference","apiVersion":"v1","reference":{"kind":"ReplicationController","namespace":"kube-system","name":"php-apache","uid":"eb9f0081-c8e5-11e6-85dd-08002750cdfb","apiVersion":"v1","resourceVersion":"26319"}}
] OwnerReferences:[{APIVersion:v1 Kind:ReplicationController Name:php-apache UID:eb9f0081-c8e5-11e6-85dd-08002750cdfb Controller:0xc421b14e4c}] Finalizers:[] ClusterName:}.
I1223 12:10:11.519449    7550 replica_set.go:196] No ReplicaSets found for pod php-apache-txnlk, ReplicaSet controller will avoid syncing
I1223 12:10:11.519458    7550 disruption.go:326] updatePod called on pod "php-apache-txnlk"
I1223 12:10:11.519466    7550 disruption.go:389] No PodDisruptionBudgets found for pod php-apache-txnlk, PodDisruptionBudget controller will avoid syncing.
I1223 12:10:11.519469    7550 disruption.go:329] No matching pdb for pod "php-apache-txnlk"
I1223 12:10:11.519479    7550 pet_set.go:239] No StatefulSets found for pod php-apache-txnlk, StatefulSet controller will avoid syncing
I1223 12:10:11.522169    7550 endpoints_controller.go:495] Update endpoints for kube-system/php-apache, ready: 2 not ready: 0
I1223 12:10:11.524448    7550 replication_controller.go:322] Observed updated replication controller php-apache. Desired pod count change: 2->2
I1223 12:10:11.524584    7550 replication_controller.go:647] Finished syncing controller "kube-system/php-apache" (5.420899ms)
I1223 12:10:11.524622    7550 controller_ref_manager.go:79] Ignoring pod kube-system/monitoring-influxdb-grafana-v3-n3odm, it's owned by [v1/ReplicationController, name: monitoring-influxdb-grafana-v3, uid: d85c1a29-c8e5-11e6-85dd-08002750cdfb]
I1223 12:10:11.524663    7550 controller_ref_manager.go:79] Ignoring pod kube-system/heapster-v1.2.0-uobac, it's owned by [v1/ReplicationController, name: heapster-v1.2.0, uid: d84f1d82-c8e5-11e6-85dd-08002750cdfb]
I1223 12:10:11.524670    7550 controller_ref_manager.go:79] Ignoring pod kube-system/kube-dns-v20-xojuw, it's owned by [v1/ReplicationController, name: kube-dns-v20, uid: 348ef075-c8de-11e6-b98e-08002750cdfb]
I1223 12:10:11.524679    7550 replication_controller.go:647] Finished syncing controller "kube-system/php-apache" (77.437µs)
I1223 12:10:11.527970    7550 endpoints_controller.go:334] Finished syncing service "kube-system/php-apache" endpoints. (9.087076ms)
I1223 12:10:11.764077    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:10:13.463004    7550 attach_detach_controller.go:539] processVolumesInUse for node "10.15.137.241"
I1223 12:10:13.768017    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:10:14.831910    7550 nodecontroller.go:713] Node 10.15.137.241 ReadyCondition updated. Updating timestamp.
I1223 12:10:14.831967    7550 nodecontroller.go:713] Node 10.15.137.242 ReadyCondition updated. Updating timestamp.
I1223 12:10:15.772032    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:10:17.776497    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:10:19.695621    7550 gc_controller.go:175] GC'ing orphaned
I1223 12:10:19.695659    7550 gc_controller.go:195] GC'ing unscheduled pods which are terminating.
I1223 12:10:19.781402    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:10:20.448103    7550 attach_detach_controller.go:539] processVolumesInUse for node "10.15.137.242"
I1223 12:10:21.786258    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:10:23.527321    7550 attach_detach_controller.go:539] processVolumesInUse for node "10.15.137.241"
I1223 12:10:23.790350    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:10:24.737485    7550 reflector.go:273] pkg/controller/volume/persistentvolume/pv_controller_base.go:455: forcing resync
I1223 12:10:24.737531    7550 reflector.go:273] pkg/controller/volume/persistentvolume/pv_controller_base.go:159: forcing resync
I1223 12:10:24.737537    7550 reflector.go:273] pkg/controller/volume/persistentvolume/pv_controller_base.go:454: forcing resync
I1223 12:10:24.834996    7550 nodecontroller.go:713] Node 10.15.137.241 ReadyCondition updated. Updating timestamp.
I1223 12:10:24.835063    7550 nodecontroller.go:713] Node 10.15.137.242 ReadyCondition updated. Updating timestamp.
I1223 12:10:25.794672    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:10:27.799287    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:10:29.803579    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:10:30.511977    7550 attach_detach_controller.go:539] processVolumesInUse for node "10.15.137.242"
I1223 12:10:31.807527    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:10:33.602084    7550 attach_detach_controller.go:539] processVolumesInUse for node "10.15.137.241"
I1223 12:10:33.811842    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:10:34.837809    7550 nodecontroller.go:713] Node 10.15.137.241 ReadyCondition updated. Updating timestamp.
I1223 12:10:34.837869    7550 nodecontroller.go:713] Node 10.15.137.242 ReadyCondition updated. Updating timestamp.
I1223 12:10:35.816310    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:10:37.821475    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:10:39.695874    7550 gc_controller.go:175] GC'ing orphaned
I1223 12:10:39.695907    7550 gc_controller.go:195] GC'ing unscheduled pods which are terminating.
I1223 12:10:39.696276    7550 reflector.go:273] pkg/controller/endpoint/endpoints_controller.go:160: forcing resync
I1223 12:10:39.696605    7550 endpoints_controller.go:334] Finished syncing service "default/kubernetes" endpoints. (1.695µs)
I1223 12:10:39.700341    7550 endpoints_controller.go:334] Finished syncing service "kube-system/php-apache" endpoints. (3.974439ms)
I1223 12:10:39.700570    7550 endpoints_controller.go:334] Finished syncing service "default/sample" endpoints. (3.947891ms)
I1223 12:10:39.700638    7550 endpoints_controller.go:334] Finished syncing service "kube-system/kube-dns" endpoints. (3.683642ms)
I1223 12:10:39.700765    7550 endpoints_controller.go:334] Finished syncing service "kube-system/heapster" endpoints. (4.06301ms)
I1223 12:10:39.700809    7550 endpoints_controller.go:334] Finished syncing service "kube-system/monitoring-grafana" endpoints. (3.704348ms)
I1223 12:10:39.701493    7550 endpoints_controller.go:334] Finished syncing service "kube-system/monitoring-influxdb" endpoints. (1.122521ms)
I1223 12:10:39.729828    7550 reflector.go:273] pkg/controller/disruption/disruption.go:283: forcing resync
I1223 12:10:39.729938    7550 reflector.go:273] pkg/controller/disruption/disruption.go:281: forcing resync
I1223 12:10:39.730022    7550 reflector.go:273] pkg/controller/disruption/disruption.go:284: forcing resync
I1223 12:10:39.730137    7550 reflector.go:273] pkg/controller/disruption/disruption.go:285: forcing resync
I1223 12:10:39.730663    7550 reflector.go:273] pkg/controller/petset/pet_set.go:148: forcing resync
I1223 12:10:39.737644    7550 reflector.go:273] pkg/controller/volume/persistentvolume/pv_controller_base.go:454: forcing resync
I1223 12:10:39.737681    7550 reflector.go:273] pkg/controller/volume/persistentvolume/pv_controller_base.go:455: forcing resync
I1223 12:10:39.737687    7550 reflector.go:273] pkg/controller/volume/persistentvolume/pv_controller_base.go:159: forcing resync
I1223 12:10:39.748872    7550 reflector.go:273] pkg/controller/informers/factory.go:89: forcing resync
I1223 12:10:39.748948    7550 deployment_controller.go:154] Updating deployment sample-deployment
I1223 12:10:39.749488    7550 deployment_controller.go:313] Finished syncing deployment "default/sample-deployment" (520.861µs)
I1223 12:10:39.825557    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:10:39.877506    7550 reflector.go:273] pkg/controller/podautoscaler/horizontal.go:133: forcing resync
I1223 12:10:40.574063    7550 attach_detach_controller.go:539] processVolumesInUse for node "10.15.137.242"
I1223 12:10:41.830190    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:10:43.666919    7550 attach_detach_controller.go:539] processVolumesInUse for node "10.15.137.241"
I1223 12:10:43.835221    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:10:44.840548    7550 nodecontroller.go:713] Node 10.15.137.241 ReadyCondition updated. Updating timestamp.
I1223 12:10:44.840607    7550 nodecontroller.go:713] Node 10.15.137.242 ReadyCondition updated. Updating timestamp.
I1223 12:10:45.839769    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:10:47.844440    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:10:49.848183    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:10:50.637178    7550 attach_detach_controller.go:539] processVolumesInUse for node "10.15.137.242"
I1223 12:10:51.854857    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:10:53.727624    7550 attach_detach_controller.go:539] processVolumesInUse for node "10.15.137.241"
I1223 12:10:53.858698    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:10:54.738004    7550 reflector.go:273] pkg/controller/volume/persistentvolume/pv_controller_base.go:159: forcing resync
I1223 12:10:54.738044    7550 reflector.go:273] pkg/controller/volume/persistentvolume/pv_controller_base.go:454: forcing resync
I1223 12:10:54.738051    7550 reflector.go:273] pkg/controller/volume/persistentvolume/pv_controller_base.go:455: forcing resync
I1223 12:10:54.843060    7550 nodecontroller.go:713] Node 10.15.137.241 ReadyCondition updated. Updating timestamp.
I1223 12:10:54.843117    7550 nodecontroller.go:713] Node 10.15.137.242 ReadyCondition updated. Updating timestamp.
I1223 12:10:55.863064    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:10:57.867452    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:10:59.696164    7550 gc_controller.go:175] GC'ing orphaned
I1223 12:10:59.696199    7550 gc_controller.go:195] GC'ing unscheduled pods which are terminating.
I1223 12:10:59.871639    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:11:00.714992    7550 attach_detach_controller.go:539] processVolumesInUse for node "10.15.137.242"
I1223 12:11:01.875782    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:11:03.792861    7550 attach_detach_controller.go:539] processVolumesInUse for node "10.15.137.241"
I1223 12:11:03.879820    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:11:04.845989    7550 nodecontroller.go:713] Node 10.15.137.241 ReadyCondition updated. Updating timestamp.
I1223 12:11:04.846050    7550 nodecontroller.go:713] Node 10.15.137.242 ReadyCondition updated. Updating timestamp.
I1223 12:11:05.884342    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:11:07.888826    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:11:09.696552    7550 reflector.go:273] pkg/controller/endpoint/endpoints_controller.go:160: forcing resync
I1223 12:11:09.697280    7550 endpoints_controller.go:334] Finished syncing service "default/kubernetes" endpoints. (1.469µs)
I1223 12:11:09.700616    7550 endpoints_controller.go:334] Finished syncing service "kube-system/monitoring-grafana" endpoints. (3.949817ms)
I1223 12:11:09.700880    7550 endpoints_controller.go:334] Finished syncing service "kube-system/monitoring-influxdb" endpoints. (4.021569ms)
I1223 12:11:09.700940    7550 endpoints_controller.go:334] Finished syncing service "default/sample" endpoints. (3.640829ms)
I1223 12:11:09.701083    7550 endpoints_controller.go:334] Finished syncing service "kube-system/php-apache" endpoints. (4.086688ms)
I1223 12:11:09.701129    7550 endpoints_controller.go:334] Finished syncing service "kube-system/heapster" endpoints. (3.694092ms)
I1223 12:11:09.701912    7550 endpoints_controller.go:334] Finished syncing service "kube-system/kube-dns" endpoints. (1.231389ms)
I1223 12:11:09.730323    7550 reflector.go:273] pkg/controller/disruption/disruption.go:285: forcing resync
I1223 12:11:09.730388    7550 reflector.go:273] pkg/controller/disruption/disruption.go:283: forcing resync
I1223 12:11:09.730421    7550 reflector.go:273] pkg/controller/disruption/disruption.go:281: forcing resync
I1223 12:11:09.730427    7550 reflector.go:273] pkg/controller/disruption/disruption.go:284: forcing resync
I1223 12:11:09.730927    7550 reflector.go:273] pkg/controller/petset/pet_set.go:148: forcing resync
I1223 12:11:09.738185    7550 reflector.go:273] pkg/controller/volume/persistentvolume/pv_controller_base.go:455: forcing resync
I1223 12:11:09.738260    7550 reflector.go:273] pkg/controller/volume/persistentvolume/pv_controller_base.go:159: forcing resync
I1223 12:11:09.738271    7550 reflector.go:273] pkg/controller/volume/persistentvolume/pv_controller_base.go:454: forcing resync
I1223 12:11:09.749023    7550 reflector.go:273] pkg/controller/informers/factory.go:89: forcing resync
I1223 12:11:09.749093    7550 deployment_controller.go:154] Updating deployment sample-deployment
I1223 12:11:09.749588    7550 deployment_controller.go:313] Finished syncing deployment "default/sample-deployment" (470.948µs)
I1223 12:11:09.877812    7550 reflector.go:273] pkg/controller/podautoscaler/horizontal.go:133: forcing resync
I1223 12:11:09.893110    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:11:10.776714    7550 attach_detach_controller.go:539] processVolumesInUse for node "10.15.137.242"
I1223 12:11:11.897159    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
I1223 12:11:13.857476    7550 attach_detach_controller.go:539] processVolumesInUse for node "10.15.137.241"
I1223 12:11:13.901514    7550 leaderelection.go:203] succesfully renewed lease kube-system/kube-controller-manager
